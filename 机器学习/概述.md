# 机器学习算法分类
1. 监督学习：输入数据有标签。 比如分类和回归。常用算法包括决策树，贝叶斯分类
2. 非监督学习：输入数据没有标签。比如聚类和关联规则学习。常用算法包括独立成分分析，K-means
3. 半监督学习：输入数据部分标签，是监督学习的延伸。常用算法包括图论推理算法。
4. 强化学习：输入数据作为对模型的反馈，强调如何基于环境而行动，以取得最大化的预期利益。与监督式学习之间的区别在于，它并不需要出现正确的输入 / 输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。


# 正则化
在模型过于复杂的情况下，模型会学习到很多特征，从而导致可能把所有训练样本都拟合到，这样就导致了过拟合。解决过拟合可以从两个方面入手，一是减少模型复杂度，一是增加训练集个数。而正则化就是减少模型复杂度的一个方法。
而这个正则化项一般会采用L1范数或者L2范数。

# 监督学习
监督学习的目标是建立一个学习过程，将预测结果与“训练数据”（即输入数据）的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率，包括分类、回归等问题。而常用算法包括线性回归、决策树、贝叶斯分类、最小二乘回归、逻辑回归、支持向量机、神经网络等。

 
# 数据集拆分
训练数据集（train dataset）：用来构建机器学习模型
验证数据集（validation dataset）：辅助构建模型，用于在构建过程中评估模型，为模型提供无偏估计，进而调整模型超参数
测试数据集（test dataset）：用来评估训练好的最终模型的性能

# 梯度下降  
梯度下降算法：在每一轮迭代均需要所有样本
mini-batch 梯度下降： 每次处理的只是单个mini-batch，而不是用全部样本。对所有训练数据进行一次遍历称之为一个epoch。比如： 假设每个mini-batch中有1000个样本，那么500万个样本一共可划分为5000个mini-batch。
因此在batch梯度下降法中，一个epoch只能完成一次梯度下降算法。但是如果在mini-batch梯度下降法中， 一个epoch可以完成5000次梯度下降算法。
随机梯度下降：  mini-batch 的一个特殊应用。SGD 等价于 b=1 的 mini-batch 梯度下降算法。即每个 mini-batch 中只有一个训练样本。

# 特征工程
特征工程是指从原始数据转换为特征向量的过程。特征工程是机器学习中最重要的起始步骤，会直接影响机器学习的效果，并通常需要大量的时间。典型的特征工程包括数据清理、特征提取、特征选择等过程。
## 数据清理 
缩放特征值（归一化）：将浮点特征值从自然范围（如 100 到 900）转换为标准范围（如 0 到 1）。特征集包含多个特征时，缩放特征可以加快梯度下降的收敛过程，并可以避免 NaN 陷阱。特征缩放的方法一般为
scaled−value=(value−mean)/stddev

处理极端离群值，如取对数、限制最大最小值等方法

分箱（离散化）

填补遗漏值

移除重复样本、不良标签、不良特征值等

平滑

正则化

## 降温 
 
高维情形下经常会碰到数据样本稀疏、距离计算困难的问题（称为 “维数灾难”），解决方法就是降维。

## 特征选择
特征选择是一个从给定的特征集合中选择与当前学习任务相关的特征的过程。特征选择中所谓的 “无关特征” 是指与当前学习任务无关，比如有一类特征称为 “冗余特征” 可以从其他特征中推演出来，它在很多时候是不起作用的，并且会增加学习过程的负担。


## 特征组合
特征组合（Feature Crosses）也称为特征交叉，指通过将两个或多个输入特征相乘来对特征空间中的非线性规律进行编码的合成特征。





https://feisky.xyz/machine-learning/basic.html

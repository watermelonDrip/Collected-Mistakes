# 逻辑回顾
逻辑回归对应线性回归，但旨在解决分类问题，即将模型的输出转换为从 0 到 1 之间的概率值。逻辑回归直接对分类的可能性进行建模，无需事先假设数据的分布。

代价函数

$min J(w) = min {-\frac{1}{m}[\sum_{i=1}^{m}y_ilog h_w (x_i) + (1-y_i)log(1-h_w(x_i))]}$
## 正则化 
在模型过于复杂的情况下，模型会学习到很多特征，从而导致可能把所有训练样本都拟合到，这样就导致了过拟合。解决过拟合可以从两个方面入手，一是减少模型复杂度，一是增加训练集个数。而正则化就是减少模型复杂度的一个方法。即以最小化损失和复杂度为目标（结构风险最小化）：
$J(w)=Loss(x,w)+\lambda Complexity(w)$
## 早停法 
这是另一种降低模型复杂度的方法。限制步数或学习概率，还还未达到最优的时候就停止迭代训练。

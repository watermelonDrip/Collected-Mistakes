{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiL+LgXzvUUucORIV0NrEU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/watermelonDrip/Collected-Mistakes/blob/main/ML2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 机器学习练习 2 - 逻辑回归\n",
        "\n",
        "在这一次练习中，我们将要实现逻辑回归并且应用到一个分类任务。我们还将通过将正则化加入训练算法，来提高算法的鲁棒性，并用更复杂的情形来测试它。"
      ],
      "metadata": {
        "id": "mp3wgPlENtzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 逻辑回归\n",
        "在训练的初始阶段，我们将要构建一个逻辑回归模型来预测，某个学生是否被大学录取。设想你是大学相关部分的管理者，想通过申请学生两次测试的评分，来决定他们是否被录取。现在你拥有之前申请学生的可以用于训练逻辑回归的训练样本集。对于每一个训练样本，你有他们两次测试的评分和最后是被录取的结果。为了完成这个预测任务，我们准备构建一个可以基于两次测试评分来评估录取可能性的分类模型。"
      ],
      "metadata": {
        "id": "dLlhtFCDN3k2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fIwPAXhZNs5H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "读数据"
      ],
      "metadata": {
        "id": "Mw8Ww70RV93P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'ex2data1.txt'\n",
        "data = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1fK_swB1V9H_",
        "outputId": "7963fdb7-e97e-4da0-f51e-341631921886"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a56457bb-1039-427b-82f6-95961cb1c3f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Exam 1</th>\n",
              "      <th>Exam 2</th>\n",
              "      <th>Admitted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34.623660</td>\n",
              "      <td>78.024693</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.286711</td>\n",
              "      <td>43.894998</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35.847409</td>\n",
              "      <td>72.902198</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60.182599</td>\n",
              "      <td>86.308552</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>79.032736</td>\n",
              "      <td>75.344376</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a56457bb-1039-427b-82f6-95961cb1c3f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a56457bb-1039-427b-82f6-95961cb1c3f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a56457bb-1039-427b-82f6-95961cb1c3f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Exam 1     Exam 2  Admitted\n",
              "0  34.623660  78.024693         0\n",
              "1  30.286711  43.894998         0\n",
              "2  35.847409  72.902198         0\n",
              "3  60.182599  86.308552         1\n",
              "4  79.032736  75.344376         1"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "让我们创建两个分数的散点图，并使用颜色编码来可视化，如果样本是正的（被接纳）或负的（未被接纳）"
      ],
      "metadata": {
        "id": "QcidjLc9WCAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive = data[data['Admitted'].isin([1])]\n",
        "negative = data[data['Admitted'].isin([0])]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')\n",
        "ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Exam 1 Score')\n",
        "ax.set_ylabel('Exam 2 Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "UEqlL2BIV_l3",
        "outputId": "a50f1e8a-a963-41cd-b9a7-8dd623952ae3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model"
      ],
      "metadata": {
        "id": "JX6FUHimXmzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim): \n",
        "        super(LogisticRegression,self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim,output_dim)\n",
        "        #  \n",
        "        self.criterion = torch.nn.BCELoss(reduction = 'mean')\n",
        "\n",
        "    def forward(self,x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "    def cal_loss(self, pred, target):\n",
        "        x = self.criterion(pred,target)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Fi_q4GDRXjxA"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "input_dim = 2 # Two inputs x1 and x2 \n",
        "output_dim = 1 # Single binary output \n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "PKSHZvFQYlOB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the Loss Function and the Optimizer\n"
      ],
      "metadata": {
        "id": "OULw8lwEZoTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(input_dim,output_dim)"
      ],
      "metadata": {
        "id": "x2eQKEQOZnyS"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#weight_decay  = 1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "gLCaOJeMYrGA"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training data"
      ],
      "metadata": {
        "id": "hpeWTfgNYxQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = data.shape[1]\n",
        "\n",
        "X = data.iloc[:,0:cols-1]\n",
        "y = data.iloc[:,cols-1:cols]\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y) "
      ],
      "metadata": {
        "id": "DTNPhibKZMMQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for test"
      ],
      "metadata": {
        "id": "fxKts9BxZRhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to numpy arrays and initalize the parameter array theta\n",
        "#X = np.array(X.values)\n",
        "#y = np.array(y.values)\n",
        "\n",
        "theta = np.zeros(2)\n",
        "X = np.array(X)\n",
        "y = np.array(y) \n",
        "# X = X.dot(theta) for test\n",
        "\n",
        "\n",
        "\n",
        "inputs = Variable(torch.from_numpy(X).cuda())\n",
        "labels = Variable(torch.from_numpy(y).cuda())\n",
        "inputs = inputs.unsqueeze(1)\n",
        "\n",
        "\n",
        "criterion = torch.nn.BCELoss(reduction = 'mean')\n",
        "m = nn.Sigmoid()\n",
        "#inputs = m(inputs)\n",
        "#v = criterion(inputs.float(),labels.float())"
      ],
      "metadata": {
        "id": "Kc5SXJozYw_g"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we convert our inputs and labels from numpy arrays to tensors.\n"
      ],
      "metadata": {
        "id": "LYO-9byDYnkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "same_seeds(1)"
      ],
      "metadata": {
        "id": "-PMISjIBrSlV"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "metadata": {
        "id": "WE98F7FbZFug"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable (Gpu)\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(X).cuda())\n",
        "        labels = Variable(torch.from_numpy(y).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(X))\n",
        "        labels = Variable(torch.from_numpy(y))\n",
        "\n",
        "\n",
        "    # get output from the model, given the inputs, # Forward pass\n",
        "    outputs = model(inputs.float())\n",
        "\n",
        "    # compute loss\n",
        "    mse_loss = model.cal_loss(outputs.float(), labels.float()) \n",
        "\n",
        "    mask = outputs.ge(0.5).float()\n",
        "    correct = (mask == labels).sum()\n",
        "    acc = correct.item()/ inputs.size(0)\n",
        " \n",
        "    # 梯度归0\n",
        "    optimizer.zero_grad()    \n",
        "    # 反向传播   \n",
        "    mse_loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss is {}'.format(epoch, mse_loss.item()))\n",
        "    print('epoch {}, acc is {}'.format(epoch, acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zViRcQ__ZgGA",
        "outputId": "503e36cc-d4ef-409f-ca0c-70a91a7d4b44"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss is 8.404232025146484\n",
            "epoch 0, acc is 0.4\n",
            "epoch 1, loss is 4.537502765655518\n",
            "epoch 1, acc is 0.37\n",
            "epoch 2, loss is 1.9488788843154907\n",
            "epoch 2, acc is 0.43\n",
            "epoch 3, loss is 1.5387624502182007\n",
            "epoch 3, acc is 0.55\n",
            "epoch 4, loss is 1.480080485343933\n",
            "epoch 4, acc is 0.53\n",
            "epoch 5, loss is 1.43136727809906\n",
            "epoch 5, acc is 0.54\n",
            "epoch 6, loss is 1.3841274976730347\n",
            "epoch 6, acc is 0.54\n",
            "epoch 7, loss is 1.3379052877426147\n",
            "epoch 7, acc is 0.54\n",
            "epoch 8, loss is 1.29276442527771\n",
            "epoch 8, acc is 0.54\n",
            "epoch 9, loss is 1.2487881183624268\n",
            "epoch 9, acc is 0.54\n",
            "epoch 10, loss is 1.2060600519180298\n",
            "epoch 10, acc is 0.54\n",
            "epoch 11, loss is 1.1646689176559448\n",
            "epoch 11, acc is 0.54\n",
            "epoch 12, loss is 1.124707579612732\n",
            "epoch 12, acc is 0.54\n",
            "epoch 13, loss is 1.0862661600112915\n",
            "epoch 13, acc is 0.54\n",
            "epoch 14, loss is 1.0494370460510254\n",
            "epoch 14, acc is 0.54\n",
            "epoch 15, loss is 1.014304757118225\n",
            "epoch 15, acc is 0.55\n",
            "epoch 16, loss is 0.9809505939483643\n",
            "epoch 16, acc is 0.56\n",
            "epoch 17, loss is 0.9494465589523315\n",
            "epoch 17, acc is 0.56\n",
            "epoch 18, loss is 0.9198518395423889\n",
            "epoch 18, acc is 0.56\n",
            "epoch 19, loss is 0.8922125697135925\n",
            "epoch 19, acc is 0.56\n",
            "epoch 20, loss is 0.8665565252304077\n",
            "epoch 20, acc is 0.55\n",
            "epoch 21, loss is 0.8428924083709717\n",
            "epoch 21, acc is 0.54\n",
            "epoch 22, loss is 0.8212081789970398\n",
            "epoch 22, acc is 0.55\n",
            "epoch 23, loss is 0.8014698624610901\n",
            "epoch 23, acc is 0.54\n",
            "epoch 24, loss is 0.7836224436759949\n",
            "epoch 24, acc is 0.54\n",
            "epoch 25, loss is 0.7675910592079163\n",
            "epoch 25, acc is 0.56\n",
            "epoch 26, loss is 0.7532839775085449\n",
            "epoch 26, acc is 0.58\n",
            "epoch 27, loss is 0.7405949234962463\n",
            "epoch 27, acc is 0.58\n",
            "epoch 28, loss is 0.7294071316719055\n",
            "epoch 28, acc is 0.59\n",
            "epoch 29, loss is 0.7195973992347717\n",
            "epoch 29, acc is 0.6\n",
            "epoch 30, loss is 0.7110398411750793\n",
            "epoch 30, acc is 0.61\n",
            "epoch 31, loss is 0.7036092877388\n",
            "epoch 31, acc is 0.61\n",
            "epoch 32, loss is 0.697184145450592\n",
            "epoch 32, acc is 0.61\n",
            "epoch 33, loss is 0.6916488409042358\n",
            "epoch 33, acc is 0.62\n",
            "epoch 34, loss is 0.6868957281112671\n",
            "epoch 34, acc is 0.61\n",
            "epoch 35, loss is 0.68282550573349\n",
            "epoch 35, acc is 0.6\n",
            "epoch 36, loss is 0.6793485879898071\n",
            "epoch 36, acc is 0.6\n",
            "epoch 37, loss is 0.6763843894004822\n",
            "epoch 37, acc is 0.6\n",
            "epoch 38, loss is 0.6738616228103638\n",
            "epoch 38, acc is 0.6\n",
            "epoch 39, loss is 0.6717173457145691\n",
            "epoch 39, acc is 0.6\n",
            "epoch 40, loss is 0.6698969602584839\n",
            "epoch 40, acc is 0.6\n",
            "epoch 41, loss is 0.6683529615402222\n",
            "epoch 41, acc is 0.6\n",
            "epoch 42, loss is 0.6670440435409546\n",
            "epoch 42, acc is 0.6\n",
            "epoch 43, loss is 0.665935218334198\n",
            "epoch 43, acc is 0.6\n",
            "epoch 44, loss is 0.6649959683418274\n",
            "epoch 44, acc is 0.6\n",
            "epoch 45, loss is 0.6642007231712341\n",
            "epoch 45, acc is 0.6\n",
            "epoch 46, loss is 0.6635273694992065\n",
            "epoch 46, acc is 0.6\n",
            "epoch 47, loss is 0.6629571914672852\n",
            "epoch 47, acc is 0.6\n",
            "epoch 48, loss is 0.6624743342399597\n",
            "epoch 48, acc is 0.6\n",
            "epoch 49, loss is 0.662065327167511\n",
            "epoch 49, acc is 0.6\n",
            "epoch 50, loss is 0.6617189049720764\n",
            "epoch 50, acc is 0.6\n",
            "epoch 51, loss is 0.6614251732826233\n",
            "epoch 51, acc is 0.6\n",
            "epoch 52, loss is 0.6611759662628174\n",
            "epoch 52, acc is 0.6\n",
            "epoch 53, loss is 0.6609645485877991\n",
            "epoch 53, acc is 0.6\n",
            "epoch 54, loss is 0.6607849597930908\n",
            "epoch 54, acc is 0.6\n",
            "epoch 55, loss is 0.660632312297821\n",
            "epoch 55, acc is 0.6\n",
            "epoch 56, loss is 0.6605024337768555\n",
            "epoch 56, acc is 0.6\n",
            "epoch 57, loss is 0.6603918075561523\n",
            "epoch 57, acc is 0.6\n",
            "epoch 58, loss is 0.6602973937988281\n",
            "epoch 58, acc is 0.6\n",
            "epoch 59, loss is 0.6602166891098022\n",
            "epoch 59, acc is 0.6\n",
            "epoch 60, loss is 0.6601476669311523\n",
            "epoch 60, acc is 0.6\n",
            "epoch 61, loss is 0.6600884795188904\n",
            "epoch 61, acc is 0.6\n",
            "epoch 62, loss is 0.6600375175476074\n",
            "epoch 62, acc is 0.6\n",
            "epoch 63, loss is 0.659993588924408\n",
            "epoch 63, acc is 0.6\n",
            "epoch 64, loss is 0.6599555611610413\n",
            "epoch 64, acc is 0.6\n",
            "epoch 65, loss is 0.6599225997924805\n",
            "epoch 65, acc is 0.6\n",
            "epoch 66, loss is 0.6598939299583435\n",
            "epoch 66, acc is 0.6\n",
            "epoch 67, loss is 0.6598687767982483\n",
            "epoch 67, acc is 0.6\n",
            "epoch 68, loss is 0.6598466634750366\n",
            "epoch 68, acc is 0.6\n",
            "epoch 69, loss is 0.6598271131515503\n",
            "epoch 69, acc is 0.6\n",
            "epoch 70, loss is 0.6598097085952759\n",
            "epoch 70, acc is 0.6\n",
            "epoch 71, loss is 0.659794270992279\n",
            "epoch 71, acc is 0.6\n",
            "epoch 72, loss is 0.6597802639007568\n",
            "epoch 72, acc is 0.6\n",
            "epoch 73, loss is 0.6597675681114197\n",
            "epoch 73, acc is 0.6\n",
            "epoch 74, loss is 0.6597561240196228\n",
            "epoch 74, acc is 0.6\n",
            "epoch 75, loss is 0.6597455739974976\n",
            "epoch 75, acc is 0.6\n",
            "epoch 76, loss is 0.6597356796264648\n",
            "epoch 76, acc is 0.6\n",
            "epoch 77, loss is 0.6597265601158142\n",
            "epoch 77, acc is 0.6\n",
            "epoch 78, loss is 0.6597180962562561\n",
            "epoch 78, acc is 0.6\n",
            "epoch 79, loss is 0.6597100496292114\n",
            "epoch 79, acc is 0.6\n",
            "epoch 80, loss is 0.6597023606300354\n",
            "epoch 80, acc is 0.6\n",
            "epoch 81, loss is 0.6596950888633728\n",
            "epoch 81, acc is 0.6\n",
            "epoch 82, loss is 0.6596879959106445\n",
            "epoch 82, acc is 0.6\n",
            "epoch 83, loss is 0.6596813201904297\n",
            "epoch 83, acc is 0.6\n",
            "epoch 84, loss is 0.6596747636795044\n",
            "epoch 84, acc is 0.6\n",
            "epoch 85, loss is 0.6596683859825134\n",
            "epoch 85, acc is 0.6\n",
            "epoch 86, loss is 0.659662127494812\n",
            "epoch 86, acc is 0.6\n",
            "epoch 87, loss is 0.6596560478210449\n",
            "epoch 87, acc is 0.6\n",
            "epoch 88, loss is 0.6596500873565674\n",
            "epoch 88, acc is 0.6\n",
            "epoch 89, loss is 0.6596441268920898\n",
            "epoch 89, acc is 0.6\n",
            "epoch 90, loss is 0.6596383452415466\n",
            "epoch 90, acc is 0.6\n",
            "epoch 91, loss is 0.6596325635910034\n",
            "epoch 91, acc is 0.6\n",
            "epoch 92, loss is 0.6596269011497498\n",
            "epoch 92, acc is 0.6\n",
            "epoch 93, loss is 0.6596212387084961\n",
            "epoch 93, acc is 0.6\n",
            "epoch 94, loss is 0.6596156358718872\n",
            "epoch 94, acc is 0.6\n",
            "epoch 95, loss is 0.6596101522445679\n",
            "epoch 95, acc is 0.6\n",
            "epoch 96, loss is 0.659604549407959\n",
            "epoch 96, acc is 0.6\n",
            "epoch 97, loss is 0.6595990061759949\n",
            "epoch 97, acc is 0.6\n",
            "epoch 98, loss is 0.6595934629440308\n",
            "epoch 98, acc is 0.6\n",
            "epoch 99, loss is 0.6595879793167114\n",
            "epoch 99, acc is 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正则化逻辑回归"
      ],
      "metadata": {
        "id": "fAK0gS84rrlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在训练的第二部分，我们将要通过加入正则项提升逻辑回归算法。如果你对正则化有点眼生，或者喜欢这一节的方程的背景，请参考在\"exercises\"文件夹中的\"ex2.pdf\"。简而言之，正则化是成本函数中的一个术语，它使算法更倾向于“更简单”的模型（在这种情况下，模型将更小的系数）。这个理论助于减少过拟合，提高模型的泛化能力。这样，我们开始吧。\n",
        "设想你是工厂的生产主管，你有一些芯片在两次测试中的测试结果。对于这两次测试，你想决定是否芯片要被接受或抛弃。为了帮助你做出艰难的决定，你拥有过去芯片的测试数据集，从其中你可以构建一个逻辑回归模型。\n"
      ],
      "metadata": {
        "id": "qHkYnhmOr7qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path =  'ex2data2.txt'\n",
        "data2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "cmXcGT1orrVt",
        "outputId": "2187cdb4-6967-48f1-c7d0-1e651ca42f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-51c75ed6-a750-4b10-8d04-2428fdef8f9c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test 1</th>\n",
              "      <th>Test 2</th>\n",
              "      <th>Accepted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.051267</td>\n",
              "      <td>0.69956</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.092742</td>\n",
              "      <td>0.68494</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.213710</td>\n",
              "      <td>0.69225</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.375000</td>\n",
              "      <td>0.50219</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.513250</td>\n",
              "      <td>0.46564</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51c75ed6-a750-4b10-8d04-2428fdef8f9c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51c75ed6-a750-4b10-8d04-2428fdef8f9c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51c75ed6-a750-4b10-8d04-2428fdef8f9c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Test 1   Test 2  Accepted\n",
              "0  0.051267  0.69956         1\n",
              "1 -0.092742  0.68494         1\n",
              "2 -0.213710  0.69225         1\n",
              "3 -0.375000  0.50219         1\n",
              "4 -0.513250  0.46564         1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive = data2[data2['Accepted'].isin([1])]\n",
        "negative = data2[data2['Accepted'].isin([0])]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.scatter(positive['Test 1'], positive['Test 2'], s=50, c='b', marker='o', label='Accepted')\n",
        "ax.scatter(negative['Test 1'], negative['Test 2'], s=50, c='r', marker='x', label='Rejected')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Test 1 Score')\n",
        "ax.set_ylabel('Test 2 Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xU_Vd4NrsIH1",
        "outputId": "204222d7-6feb-4b33-a613-a685db0c12bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "哇，这个数据看起来可比前一次的复杂得多。特别地，你会注意到其中没有线性决策界限，来良好的分开两类数据。一个方法是用像逻辑回归这样的线性技术来构造从原始特征的多项式中得到的特征。让我们通过创建一组多项式特征入手吧。"
      ],
      "metadata": {
        "id": "kiUBr075sK9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degree = 5\n",
        "x1 = data2['Test 1']\n",
        "x2 = data2['Test 2']\n",
        "\n",
        "data2.insert(3, 'Ones', 1)\n",
        "\n",
        "for i in range(1, degree):\n",
        "    for j in range(0, i):\n",
        "        data2['F' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)\n",
        "\n",
        "data2.drop('Test 1', axis=1, inplace=True)\n",
        "data2.drop('Test 2', axis=1, inplace=True)\n",
        "\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "rttFoR1_sSkV",
        "outputId": "b6c5f139-9538-4943-ff39-b4baa95740af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d96125fc-367c-49af-a085-6e316c629ccc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accepted</th>\n",
              "      <th>Ones</th>\n",
              "      <th>F10</th>\n",
              "      <th>F20</th>\n",
              "      <th>F21</th>\n",
              "      <th>F30</th>\n",
              "      <th>F31</th>\n",
              "      <th>F32</th>\n",
              "      <th>F40</th>\n",
              "      <th>F41</th>\n",
              "      <th>F42</th>\n",
              "      <th>F43</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.051267</td>\n",
              "      <td>0.002628</td>\n",
              "      <td>0.035864</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>0.025089</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.001286</td>\n",
              "      <td>0.017551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.092742</td>\n",
              "      <td>0.008601</td>\n",
              "      <td>-0.063523</td>\n",
              "      <td>-0.000798</td>\n",
              "      <td>0.005891</td>\n",
              "      <td>-0.043509</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>-0.000546</td>\n",
              "      <td>0.004035</td>\n",
              "      <td>-0.029801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.213710</td>\n",
              "      <td>0.045672</td>\n",
              "      <td>-0.147941</td>\n",
              "      <td>-0.009761</td>\n",
              "      <td>0.031616</td>\n",
              "      <td>-0.102412</td>\n",
              "      <td>0.002086</td>\n",
              "      <td>-0.006757</td>\n",
              "      <td>0.021886</td>\n",
              "      <td>-0.070895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.375000</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>-0.188321</td>\n",
              "      <td>-0.052734</td>\n",
              "      <td>0.070620</td>\n",
              "      <td>-0.094573</td>\n",
              "      <td>0.019775</td>\n",
              "      <td>-0.026483</td>\n",
              "      <td>0.035465</td>\n",
              "      <td>-0.047494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.513250</td>\n",
              "      <td>0.263426</td>\n",
              "      <td>-0.238990</td>\n",
              "      <td>-0.135203</td>\n",
              "      <td>0.122661</td>\n",
              "      <td>-0.111283</td>\n",
              "      <td>0.069393</td>\n",
              "      <td>-0.062956</td>\n",
              "      <td>0.057116</td>\n",
              "      <td>-0.051818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d96125fc-367c-49af-a085-6e316c629ccc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d96125fc-367c-49af-a085-6e316c629ccc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d96125fc-367c-49af-a085-6e316c629ccc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Accepted  Ones       F10       F20  ...       F40       F41       F42       F43\n",
              "0         1     1  0.051267  0.002628  ...  0.000007  0.000094  0.001286  0.017551\n",
              "1         1     1 -0.092742  0.008601  ...  0.000074 -0.000546  0.004035 -0.029801\n",
              "2         1     1 -0.213710  0.045672  ...  0.002086 -0.006757  0.021886 -0.070895\n",
              "3         1     1 -0.375000  0.140625  ...  0.019775 -0.026483  0.035465 -0.047494\n",
              "4         1     1 -0.513250  0.263426  ...  0.069393 -0.062956  0.057116 -0.051818\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在，我们需要修改第1部分的成本和梯度函数，包括正则化项。首先是成本函数：\n",
        "# regularized cost（正则化代价函数）\n",
        "$$J\\left( \\theta  \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[-{{y}^{(i)}}\\log \\left( {{h}_{\\theta }}\\left( {{x}^{(i)}} \\right) \\right)-\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{{h}_{\\theta }}\\left( {{x}^{(i)}} \\right) \\right)]}+\\frac{\\lambda }{2m}\\sum\\limits_{j=1}^{n}{\\theta _{j}^{2}}$$"
      ],
      "metadata": {
        "id": "eMvHU-WYshvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对${{\\theta }_{0}}$ 进行正则化，所以梯度下降算法将分两种情形：\n",
        "\\begin{align}\n",
        "  & Repeat\\text{ }until\\text{ }convergence\\text{ }\\!\\!\\{\\!\\!\\text{ } \\\\ \n",
        " & \\text{     }{{\\theta }_{0}}:={{\\theta }_{0}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[{{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}}]x_{_{0}}^{(i)}} \\\\ \n",
        " & \\text{     }{{\\theta }_{j}}:={{\\theta }_{j}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[{{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}}]x_{j}^{(i)}}+\\frac{\\lambda }{m}{{\\theta }_{j}} \\\\ \n",
        " & \\text{          }\\!\\!\\}\\!\\!\\text{ } \\\\ \n",
        " & Repeat \\\\ \n",
        "\\end{align}\n",
        "\n",
        "对上面的算法中 j=1,2,...,n 时的更新式子进行调整可得： \n",
        "${{\\theta }_{j}}:={{\\theta }_{j}}(1-a\\frac{\\lambda }{m})-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}})x_{j}^{(i)}}$"
      ],
      "metadata": {
        "id": "8lQDGL7vsqem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "首先初始化变量"
      ],
      "metadata": {
        "id": "mVXWma-sswCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set X and y (remember from above that we moved the label to column 0)\n",
        "cols = data2.shape[1]\n",
        "X2 = data2.iloc[:,1:cols]\n",
        "y2 = data2.iloc[:,0:1]\n",
        "\n",
        "# convert to numpy arrays and initalize the parameter array theta\n",
        "X2 = np.array(X2.values)\n",
        "y2 = np.array(y2.values)\n",
        "theta2 = np.zeros(11)"
      ],
      "metadata": {
        "id": "u_LX3lLYsimL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2.shape,   y2.shape"
      ],
      "metadata": {
        "id": "3x7jSHs8s6F1",
        "outputId": "59ccc5e9-cbe3-4f02-f9c2-dce3a58cbea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((118, 11), (118, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cost function"
      ],
      "metadata": {
        "id": "oMw4yMS9slQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.BCELoss(reduction = 'mean')"
      ],
      "metadata": {
        "id": "xSn7Vebqw9kx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learningRate = 1"
      ],
      "metadata": {
        "id": "2ZQTZGPSxguS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionReg(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim): \n",
        "        super(LogisticRegressionReg,self).__init__()\n",
        "        self.poly = torch.nn.Linear(input_dim,output_dim)\n",
        "        #  \n",
        "        self.criterion = torch.nn.BCELoss(reduction = 'mean')\n",
        "\n",
        "    def forward(self,x):\n",
        "        y_pred = torch.sigmoid(self.poly(x))\n",
        "        return y_pred\n",
        "    def cal_loss(self, pred, target):\n",
        "        x = self.criterion(pred,target)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WbZz2vjcxwDb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 11\n",
        "output_dim = 1\n",
        "learning_rate = 1\n",
        "epochs = 1000\n",
        "model = LogisticRegressionReg(input_dim,output_dim)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.00001 )\n"
      ],
      "metadata": {
        "id": "-Ql-ApE9x8U7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "metadata": {
        "id": "lwA09vxe0aS7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable (Gpu)\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(X2).cuda())\n",
        "        labels = Variable(torch.from_numpy(y2).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(X2))\n",
        "        labels = Variable(torch.from_numpy(y2))\n",
        "\n",
        "\n",
        "    # get output from the model, given the inputs, # Forward pass\n",
        "    outputs = model(inputs.float())\n",
        "\n",
        "    # compute loss\n",
        "    mse_loss = model.cal_loss(outputs.float(), labels.float()) \n",
        "\n",
        "\n",
        " \n",
        "    # 梯度归0\n",
        "    optimizer.zero_grad()    \n",
        "    # 反向传播   \n",
        "    mse_loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    mask = outputs.ge(0.5).float()\n",
        "    correct = (mask == labels).sum()\n",
        "    acc = correct.item()/ inputs.size(0)\n",
        "    \n",
        "    print('epoch {}, loss is {}'.format(epoch, mse_loss.item()))\n",
        "    print('epoch {}, acc is {}'.format(epoch, acc))"
      ],
      "metadata": {
        "id": "-suzjp6axmMZ",
        "outputId": "f4d38e41-f2b5-45e4-db55-745539c648f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss is 0.5924578905105591\n",
            "epoch 0, acc is 0.652542372881356\n",
            "epoch 1, loss is 0.592432975769043\n",
            "epoch 1, acc is 0.652542372881356\n",
            "epoch 2, loss is 0.5924078822135925\n",
            "epoch 2, acc is 0.652542372881356\n",
            "epoch 3, loss is 0.5923829078674316\n",
            "epoch 3, acc is 0.652542372881356\n",
            "epoch 4, loss is 0.5923579335212708\n",
            "epoch 4, acc is 0.652542372881356\n",
            "epoch 5, loss is 0.5923331379890442\n",
            "epoch 5, acc is 0.652542372881356\n",
            "epoch 6, loss is 0.5923082828521729\n",
            "epoch 6, acc is 0.652542372881356\n",
            "epoch 7, loss is 0.5922834873199463\n",
            "epoch 7, acc is 0.652542372881356\n",
            "epoch 8, loss is 0.592258632183075\n",
            "epoch 8, acc is 0.652542372881356\n",
            "epoch 9, loss is 0.5922339558601379\n",
            "epoch 9, acc is 0.652542372881356\n",
            "epoch 10, loss is 0.5922093391418457\n",
            "epoch 10, acc is 0.652542372881356\n",
            "epoch 11, loss is 0.5921846628189087\n",
            "epoch 11, acc is 0.652542372881356\n",
            "epoch 12, loss is 0.5921601057052612\n",
            "epoch 12, acc is 0.652542372881356\n",
            "epoch 13, loss is 0.592135488986969\n",
            "epoch 13, acc is 0.652542372881356\n",
            "epoch 14, loss is 0.5921109914779663\n",
            "epoch 14, acc is 0.652542372881356\n",
            "epoch 15, loss is 0.5920865535736084\n",
            "epoch 15, acc is 0.652542372881356\n",
            "epoch 16, loss is 0.5920621156692505\n",
            "epoch 16, acc is 0.652542372881356\n",
            "epoch 17, loss is 0.5920377373695374\n",
            "epoch 17, acc is 0.652542372881356\n",
            "epoch 18, loss is 0.5920133590698242\n",
            "epoch 18, acc is 0.652542372881356\n",
            "epoch 19, loss is 0.5919890403747559\n",
            "epoch 19, acc is 0.652542372881356\n",
            "epoch 20, loss is 0.5919647216796875\n",
            "epoch 20, acc is 0.652542372881356\n",
            "epoch 21, loss is 0.5919405221939087\n",
            "epoch 21, acc is 0.652542372881356\n",
            "epoch 22, loss is 0.5919163823127747\n",
            "epoch 22, acc is 0.652542372881356\n",
            "epoch 23, loss is 0.5918921828269958\n",
            "epoch 23, acc is 0.652542372881356\n",
            "epoch 24, loss is 0.5918680429458618\n",
            "epoch 24, acc is 0.652542372881356\n",
            "epoch 25, loss is 0.5918440222740173\n",
            "epoch 25, acc is 0.652542372881356\n",
            "epoch 26, loss is 0.5918200016021729\n",
            "epoch 26, acc is 0.652542372881356\n",
            "epoch 27, loss is 0.5917959809303284\n",
            "epoch 27, acc is 0.652542372881356\n",
            "epoch 28, loss is 0.5917719602584839\n",
            "epoch 28, acc is 0.652542372881356\n",
            "epoch 29, loss is 0.5917481184005737\n",
            "epoch 29, acc is 0.652542372881356\n",
            "epoch 30, loss is 0.5917242169380188\n",
            "epoch 30, acc is 0.652542372881356\n",
            "epoch 31, loss is 0.5917004346847534\n",
            "epoch 31, acc is 0.652542372881356\n",
            "epoch 32, loss is 0.5916765928268433\n",
            "epoch 32, acc is 0.652542372881356\n",
            "epoch 33, loss is 0.5916528105735779\n",
            "epoch 33, acc is 0.652542372881356\n",
            "epoch 34, loss is 0.591629147529602\n",
            "epoch 34, acc is 0.652542372881356\n",
            "epoch 35, loss is 0.5916054844856262\n",
            "epoch 35, acc is 0.652542372881356\n",
            "epoch 36, loss is 0.5915818214416504\n",
            "epoch 36, acc is 0.652542372881356\n",
            "epoch 37, loss is 0.5915582180023193\n",
            "epoch 37, acc is 0.652542372881356\n",
            "epoch 38, loss is 0.5915346145629883\n",
            "epoch 38, acc is 0.652542372881356\n",
            "epoch 39, loss is 0.591511070728302\n",
            "epoch 39, acc is 0.652542372881356\n",
            "epoch 40, loss is 0.5914876461029053\n",
            "epoch 40, acc is 0.652542372881356\n",
            "epoch 41, loss is 0.5914641618728638\n",
            "epoch 41, acc is 0.652542372881356\n",
            "epoch 42, loss is 0.591440737247467\n",
            "epoch 42, acc is 0.652542372881356\n",
            "epoch 43, loss is 0.5914173722267151\n",
            "epoch 43, acc is 0.652542372881356\n",
            "epoch 44, loss is 0.5913940668106079\n",
            "epoch 44, acc is 0.652542372881356\n",
            "epoch 45, loss is 0.5913708209991455\n",
            "epoch 45, acc is 0.652542372881356\n",
            "epoch 46, loss is 0.5913475155830383\n",
            "epoch 46, acc is 0.652542372881356\n",
            "epoch 47, loss is 0.5913243293762207\n",
            "epoch 47, acc is 0.652542372881356\n",
            "epoch 48, loss is 0.5913010835647583\n",
            "epoch 48, acc is 0.652542372881356\n",
            "epoch 49, loss is 0.5912779569625854\n",
            "epoch 49, acc is 0.652542372881356\n",
            "epoch 50, loss is 0.5912548899650574\n",
            "epoch 50, acc is 0.652542372881356\n",
            "epoch 51, loss is 0.5912317633628845\n",
            "epoch 51, acc is 0.652542372881356\n",
            "epoch 52, loss is 0.5912086963653564\n",
            "epoch 52, acc is 0.652542372881356\n",
            "epoch 53, loss is 0.5911857485771179\n",
            "epoch 53, acc is 0.652542372881356\n",
            "epoch 54, loss is 0.5911627411842346\n",
            "epoch 54, acc is 0.652542372881356\n",
            "epoch 55, loss is 0.5911398530006409\n",
            "epoch 55, acc is 0.652542372881356\n",
            "epoch 56, loss is 0.5911169648170471\n",
            "epoch 56, acc is 0.652542372881356\n",
            "epoch 57, loss is 0.5910940766334534\n",
            "epoch 57, acc is 0.652542372881356\n",
            "epoch 58, loss is 0.5910713076591492\n",
            "epoch 58, acc is 0.652542372881356\n",
            "epoch 59, loss is 0.5910484790802002\n",
            "epoch 59, acc is 0.652542372881356\n",
            "epoch 60, loss is 0.591025710105896\n",
            "epoch 60, acc is 0.652542372881356\n",
            "epoch 61, loss is 0.5910030603408813\n",
            "epoch 61, acc is 0.652542372881356\n",
            "epoch 62, loss is 0.5909804105758667\n",
            "epoch 62, acc is 0.652542372881356\n",
            "epoch 63, loss is 0.590957760810852\n",
            "epoch 63, acc is 0.652542372881356\n",
            "epoch 64, loss is 0.5909351706504822\n",
            "epoch 64, acc is 0.652542372881356\n",
            "epoch 65, loss is 0.5909125804901123\n",
            "epoch 65, acc is 0.652542372881356\n",
            "epoch 66, loss is 0.5908900499343872\n",
            "epoch 66, acc is 0.652542372881356\n",
            "epoch 67, loss is 0.5908675193786621\n",
            "epoch 67, acc is 0.652542372881356\n",
            "epoch 68, loss is 0.5908451676368713\n",
            "epoch 68, acc is 0.652542372881356\n",
            "epoch 69, loss is 0.5908226370811462\n",
            "epoch 69, acc is 0.652542372881356\n",
            "epoch 70, loss is 0.5908002853393555\n",
            "epoch 70, acc is 0.652542372881356\n",
            "epoch 71, loss is 0.5907778739929199\n",
            "epoch 71, acc is 0.652542372881356\n",
            "epoch 72, loss is 0.5907556414604187\n",
            "epoch 72, acc is 0.652542372881356\n",
            "epoch 73, loss is 0.5907334089279175\n",
            "epoch 73, acc is 0.652542372881356\n",
            "epoch 74, loss is 0.5907111167907715\n",
            "epoch 74, acc is 0.652542372881356\n",
            "epoch 75, loss is 0.590688943862915\n",
            "epoch 75, acc is 0.652542372881356\n",
            "epoch 76, loss is 0.5906667709350586\n",
            "epoch 76, acc is 0.652542372881356\n",
            "epoch 77, loss is 0.5906445980072021\n",
            "epoch 77, acc is 0.652542372881356\n",
            "epoch 78, loss is 0.5906224846839905\n",
            "epoch 78, acc is 0.652542372881356\n",
            "epoch 79, loss is 0.5906004309654236\n",
            "epoch 79, acc is 0.652542372881356\n",
            "epoch 80, loss is 0.5905783772468567\n",
            "epoch 80, acc is 0.652542372881356\n",
            "epoch 81, loss is 0.5905563831329346\n",
            "epoch 81, acc is 0.652542372881356\n",
            "epoch 82, loss is 0.5905344486236572\n",
            "epoch 82, acc is 0.652542372881356\n",
            "epoch 83, loss is 0.5905125737190247\n",
            "epoch 83, acc is 0.652542372881356\n",
            "epoch 84, loss is 0.5904905796051025\n",
            "epoch 84, acc is 0.652542372881356\n",
            "epoch 85, loss is 0.59046870470047\n",
            "epoch 85, acc is 0.652542372881356\n",
            "epoch 86, loss is 0.5904468894004822\n",
            "epoch 86, acc is 0.652542372881356\n",
            "epoch 87, loss is 0.5904251337051392\n",
            "epoch 87, acc is 0.652542372881356\n",
            "epoch 88, loss is 0.5904034376144409\n",
            "epoch 88, acc is 0.652542372881356\n",
            "epoch 89, loss is 0.5903816223144531\n",
            "epoch 89, acc is 0.652542372881356\n",
            "epoch 90, loss is 0.5903599858283997\n",
            "epoch 90, acc is 0.652542372881356\n",
            "epoch 91, loss is 0.5903382301330566\n",
            "epoch 91, acc is 0.652542372881356\n",
            "epoch 92, loss is 0.590316653251648\n",
            "epoch 92, acc is 0.652542372881356\n",
            "epoch 93, loss is 0.590295135974884\n",
            "epoch 93, acc is 0.652542372881356\n",
            "epoch 94, loss is 0.5902735590934753\n",
            "epoch 94, acc is 0.652542372881356\n",
            "epoch 95, loss is 0.5902519822120667\n",
            "epoch 95, acc is 0.652542372881356\n",
            "epoch 96, loss is 0.5902305245399475\n",
            "epoch 96, acc is 0.652542372881356\n",
            "epoch 97, loss is 0.5902090668678284\n",
            "epoch 97, acc is 0.652542372881356\n",
            "epoch 98, loss is 0.5901876091957092\n",
            "epoch 98, acc is 0.652542372881356\n",
            "epoch 99, loss is 0.5901662707328796\n",
            "epoch 99, acc is 0.652542372881356\n",
            "epoch 100, loss is 0.5901448726654053\n",
            "epoch 100, acc is 0.652542372881356\n",
            "epoch 101, loss is 0.5901235938072205\n",
            "epoch 101, acc is 0.652542372881356\n",
            "epoch 102, loss is 0.5901022553443909\n",
            "epoch 102, acc is 0.652542372881356\n",
            "epoch 103, loss is 0.590080976486206\n",
            "epoch 103, acc is 0.652542372881356\n",
            "epoch 104, loss is 0.5900596976280212\n",
            "epoch 104, acc is 0.652542372881356\n",
            "epoch 105, loss is 0.590038537979126\n",
            "epoch 105, acc is 0.652542372881356\n",
            "epoch 106, loss is 0.5900174379348755\n",
            "epoch 106, acc is 0.652542372881356\n",
            "epoch 107, loss is 0.5899962782859802\n",
            "epoch 107, acc is 0.652542372881356\n",
            "epoch 108, loss is 0.589975118637085\n",
            "epoch 108, acc is 0.652542372881356\n",
            "epoch 109, loss is 0.589954137802124\n",
            "epoch 109, acc is 0.652542372881356\n",
            "epoch 110, loss is 0.5899330973625183\n",
            "epoch 110, acc is 0.652542372881356\n",
            "epoch 111, loss is 0.5899120569229126\n",
            "epoch 111, acc is 0.652542372881356\n",
            "epoch 112, loss is 0.5898910760879517\n",
            "epoch 112, acc is 0.652542372881356\n",
            "epoch 113, loss is 0.5898702144622803\n",
            "epoch 113, acc is 0.652542372881356\n",
            "epoch 114, loss is 0.5898492932319641\n",
            "epoch 114, acc is 0.652542372881356\n",
            "epoch 115, loss is 0.589828372001648\n",
            "epoch 115, acc is 0.652542372881356\n",
            "epoch 116, loss is 0.5898075699806213\n",
            "epoch 116, acc is 0.652542372881356\n",
            "epoch 117, loss is 0.58978670835495\n",
            "epoch 117, acc is 0.652542372881356\n",
            "epoch 118, loss is 0.5897660255432129\n",
            "epoch 118, acc is 0.652542372881356\n",
            "epoch 119, loss is 0.5897452235221863\n",
            "epoch 119, acc is 0.652542372881356\n",
            "epoch 120, loss is 0.5897245407104492\n",
            "epoch 120, acc is 0.652542372881356\n",
            "epoch 121, loss is 0.5897038578987122\n",
            "epoch 121, acc is 0.652542372881356\n",
            "epoch 122, loss is 0.5896831750869751\n",
            "epoch 122, acc is 0.652542372881356\n",
            "epoch 123, loss is 0.5896626114845276\n",
            "epoch 123, acc is 0.652542372881356\n",
            "epoch 124, loss is 0.5896419882774353\n",
            "epoch 124, acc is 0.652542372881356\n",
            "epoch 125, loss is 0.5896214246749878\n",
            "epoch 125, acc is 0.652542372881356\n",
            "epoch 126, loss is 0.5896009206771851\n",
            "epoch 126, acc is 0.652542372881356\n",
            "epoch 127, loss is 0.5895803570747375\n",
            "epoch 127, acc is 0.652542372881356\n",
            "epoch 128, loss is 0.5895599126815796\n",
            "epoch 128, acc is 0.652542372881356\n",
            "epoch 129, loss is 0.5895394682884216\n",
            "epoch 129, acc is 0.652542372881356\n",
            "epoch 130, loss is 0.5895190238952637\n",
            "epoch 130, acc is 0.652542372881356\n",
            "epoch 131, loss is 0.58949875831604\n",
            "epoch 131, acc is 0.652542372881356\n",
            "epoch 132, loss is 0.5894783735275269\n",
            "epoch 132, acc is 0.652542372881356\n",
            "epoch 133, loss is 0.5894579887390137\n",
            "epoch 133, acc is 0.652542372881356\n",
            "epoch 134, loss is 0.5894377827644348\n",
            "epoch 134, acc is 0.652542372881356\n",
            "epoch 135, loss is 0.5894175171852112\n",
            "epoch 135, acc is 0.652542372881356\n",
            "epoch 136, loss is 0.5893973708152771\n",
            "epoch 136, acc is 0.652542372881356\n",
            "epoch 137, loss is 0.5893771052360535\n",
            "epoch 137, acc is 0.652542372881356\n",
            "epoch 138, loss is 0.5893569588661194\n",
            "epoch 138, acc is 0.652542372881356\n",
            "epoch 139, loss is 0.5893368721008301\n",
            "epoch 139, acc is 0.652542372881356\n",
            "epoch 140, loss is 0.589316725730896\n",
            "epoch 140, acc is 0.652542372881356\n",
            "epoch 141, loss is 0.5892966389656067\n",
            "epoch 141, acc is 0.652542372881356\n",
            "epoch 142, loss is 0.5892766118049622\n",
            "epoch 142, acc is 0.652542372881356\n",
            "epoch 143, loss is 0.5892565846443176\n",
            "epoch 143, acc is 0.652542372881356\n",
            "epoch 144, loss is 0.5892366170883179\n",
            "epoch 144, acc is 0.652542372881356\n",
            "epoch 145, loss is 0.5892167091369629\n",
            "epoch 145, acc is 0.652542372881356\n",
            "epoch 146, loss is 0.5891968011856079\n",
            "epoch 146, acc is 0.652542372881356\n",
            "epoch 147, loss is 0.5891768932342529\n",
            "epoch 147, acc is 0.652542372881356\n",
            "epoch 148, loss is 0.589156985282898\n",
            "epoch 148, acc is 0.652542372881356\n",
            "epoch 149, loss is 0.5891371965408325\n",
            "epoch 149, acc is 0.652542372881356\n",
            "epoch 150, loss is 0.5891174077987671\n",
            "epoch 150, acc is 0.652542372881356\n",
            "epoch 151, loss is 0.5890976190567017\n",
            "epoch 151, acc is 0.652542372881356\n",
            "epoch 152, loss is 0.5890778303146362\n",
            "epoch 152, acc is 0.652542372881356\n",
            "epoch 153, loss is 0.5890581011772156\n",
            "epoch 153, acc is 0.652542372881356\n",
            "epoch 154, loss is 0.5890383720397949\n",
            "epoch 154, acc is 0.652542372881356\n",
            "epoch 155, loss is 0.5890187621116638\n",
            "epoch 155, acc is 0.652542372881356\n",
            "epoch 156, loss is 0.5889991521835327\n",
            "epoch 156, acc is 0.652542372881356\n",
            "epoch 157, loss is 0.5889795422554016\n",
            "epoch 157, acc is 0.652542372881356\n",
            "epoch 158, loss is 0.5889599919319153\n",
            "epoch 158, acc is 0.652542372881356\n",
            "epoch 159, loss is 0.588940441608429\n",
            "epoch 159, acc is 0.652542372881356\n",
            "epoch 160, loss is 0.5889208912849426\n",
            "epoch 160, acc is 0.652542372881356\n",
            "epoch 161, loss is 0.5889014601707458\n",
            "epoch 161, acc is 0.652542372881356\n",
            "epoch 162, loss is 0.5888819098472595\n",
            "epoch 162, acc is 0.652542372881356\n",
            "epoch 163, loss is 0.5888625383377075\n",
            "epoch 163, acc is 0.652542372881356\n",
            "epoch 164, loss is 0.5888431072235107\n",
            "epoch 164, acc is 0.652542372881356\n",
            "epoch 165, loss is 0.5888237953186035\n",
            "epoch 165, acc is 0.652542372881356\n",
            "epoch 166, loss is 0.5888044238090515\n",
            "epoch 166, acc is 0.652542372881356\n",
            "epoch 167, loss is 0.5887851119041443\n",
            "epoch 167, acc is 0.652542372881356\n",
            "epoch 168, loss is 0.5887657999992371\n",
            "epoch 168, acc is 0.652542372881356\n",
            "epoch 169, loss is 0.5887465476989746\n",
            "epoch 169, acc is 0.652542372881356\n",
            "epoch 170, loss is 0.5887274146080017\n",
            "epoch 170, acc is 0.652542372881356\n",
            "epoch 171, loss is 0.5887081623077393\n",
            "epoch 171, acc is 0.652542372881356\n",
            "epoch 172, loss is 0.5886889100074768\n",
            "epoch 172, acc is 0.652542372881356\n",
            "epoch 173, loss is 0.5886697769165039\n",
            "epoch 173, acc is 0.652542372881356\n",
            "epoch 174, loss is 0.5886507034301758\n",
            "epoch 174, acc is 0.652542372881356\n",
            "epoch 175, loss is 0.5886315703392029\n",
            "epoch 175, acc is 0.652542372881356\n",
            "epoch 176, loss is 0.58861243724823\n",
            "epoch 176, acc is 0.652542372881356\n",
            "epoch 177, loss is 0.5885934829711914\n",
            "epoch 177, acc is 0.652542372881356\n",
            "epoch 178, loss is 0.5885744094848633\n",
            "epoch 178, acc is 0.652542372881356\n",
            "epoch 179, loss is 0.5885555744171143\n",
            "epoch 179, acc is 0.652542372881356\n",
            "epoch 180, loss is 0.5885365605354309\n",
            "epoch 180, acc is 0.652542372881356\n",
            "epoch 181, loss is 0.5885176658630371\n",
            "epoch 181, acc is 0.652542372881356\n",
            "epoch 182, loss is 0.5884987115859985\n",
            "epoch 182, acc is 0.652542372881356\n",
            "epoch 183, loss is 0.5884798169136047\n",
            "epoch 183, acc is 0.652542372881356\n",
            "epoch 184, loss is 0.5884610414505005\n",
            "epoch 184, acc is 0.652542372881356\n",
            "epoch 185, loss is 0.5884421467781067\n",
            "epoch 185, acc is 0.652542372881356\n",
            "epoch 186, loss is 0.5884233713150024\n",
            "epoch 186, acc is 0.652542372881356\n",
            "epoch 187, loss is 0.588404655456543\n",
            "epoch 187, acc is 0.652542372881356\n",
            "epoch 188, loss is 0.5883858799934387\n",
            "epoch 188, acc is 0.652542372881356\n",
            "epoch 189, loss is 0.5883671641349792\n",
            "epoch 189, acc is 0.652542372881356\n",
            "epoch 190, loss is 0.5883485078811646\n",
            "epoch 190, acc is 0.652542372881356\n",
            "epoch 191, loss is 0.5883297920227051\n",
            "epoch 191, acc is 0.652542372881356\n",
            "epoch 192, loss is 0.5883111357688904\n",
            "epoch 192, acc is 0.652542372881356\n",
            "epoch 193, loss is 0.5882925987243652\n",
            "epoch 193, acc is 0.652542372881356\n",
            "epoch 194, loss is 0.5882739424705505\n",
            "epoch 194, acc is 0.652542372881356\n",
            "epoch 195, loss is 0.5882554054260254\n",
            "epoch 195, acc is 0.652542372881356\n",
            "epoch 196, loss is 0.588236927986145\n",
            "epoch 196, acc is 0.652542372881356\n",
            "epoch 197, loss is 0.5882184505462646\n",
            "epoch 197, acc is 0.652542372881356\n",
            "epoch 198, loss is 0.5881999135017395\n",
            "epoch 198, acc is 0.652542372881356\n",
            "epoch 199, loss is 0.5881814360618591\n",
            "epoch 199, acc is 0.652542372881356\n",
            "epoch 200, loss is 0.5881630182266235\n",
            "epoch 200, acc is 0.652542372881356\n",
            "epoch 201, loss is 0.5881446599960327\n",
            "epoch 201, acc is 0.652542372881356\n",
            "epoch 202, loss is 0.5881263017654419\n",
            "epoch 202, acc is 0.652542372881356\n",
            "epoch 203, loss is 0.5881079435348511\n",
            "epoch 203, acc is 0.652542372881356\n",
            "epoch 204, loss is 0.588089644908905\n",
            "epoch 204, acc is 0.652542372881356\n",
            "epoch 205, loss is 0.588071346282959\n",
            "epoch 205, acc is 0.652542372881356\n",
            "epoch 206, loss is 0.5880530476570129\n",
            "epoch 206, acc is 0.652542372881356\n",
            "epoch 207, loss is 0.5880348086357117\n",
            "epoch 207, acc is 0.652542372881356\n",
            "epoch 208, loss is 0.5880165696144104\n",
            "epoch 208, acc is 0.652542372881356\n",
            "epoch 209, loss is 0.5879983901977539\n",
            "epoch 209, acc is 0.652542372881356\n",
            "epoch 210, loss is 0.5879802107810974\n",
            "epoch 210, acc is 0.652542372881356\n",
            "epoch 211, loss is 0.5879621505737305\n",
            "epoch 211, acc is 0.652542372881356\n",
            "epoch 212, loss is 0.587943971157074\n",
            "epoch 212, acc is 0.652542372881356\n",
            "epoch 213, loss is 0.5879258513450623\n",
            "epoch 213, acc is 0.652542372881356\n",
            "epoch 214, loss is 0.5879078507423401\n",
            "epoch 214, acc is 0.652542372881356\n",
            "epoch 215, loss is 0.5878897905349731\n",
            "epoch 215, acc is 0.652542372881356\n",
            "epoch 216, loss is 0.5878717303276062\n",
            "epoch 216, acc is 0.652542372881356\n",
            "epoch 217, loss is 0.5878537893295288\n",
            "epoch 217, acc is 0.652542372881356\n",
            "epoch 218, loss is 0.5878357887268066\n",
            "epoch 218, acc is 0.652542372881356\n",
            "epoch 219, loss is 0.587817907333374\n",
            "epoch 219, acc is 0.652542372881356\n",
            "epoch 220, loss is 0.5877999663352966\n",
            "epoch 220, acc is 0.652542372881356\n",
            "epoch 221, loss is 0.587782084941864\n",
            "epoch 221, acc is 0.652542372881356\n",
            "epoch 222, loss is 0.5877641439437866\n",
            "epoch 222, acc is 0.652542372881356\n",
            "epoch 223, loss is 0.5877463817596436\n",
            "epoch 223, acc is 0.652542372881356\n",
            "epoch 224, loss is 0.5877286195755005\n",
            "epoch 224, acc is 0.652542372881356\n",
            "epoch 225, loss is 0.5877107381820679\n",
            "epoch 225, acc is 0.652542372881356\n",
            "epoch 226, loss is 0.5876930356025696\n",
            "epoch 226, acc is 0.652542372881356\n",
            "epoch 227, loss is 0.5876752138137817\n",
            "epoch 227, acc is 0.652542372881356\n",
            "epoch 228, loss is 0.587657630443573\n",
            "epoch 228, acc is 0.652542372881356\n",
            "epoch 229, loss is 0.5876398086547852\n",
            "epoch 229, acc is 0.652542372881356\n",
            "epoch 230, loss is 0.5876221656799316\n",
            "epoch 230, acc is 0.652542372881356\n",
            "epoch 231, loss is 0.5876045823097229\n",
            "epoch 231, acc is 0.652542372881356\n",
            "epoch 232, loss is 0.5875869393348694\n",
            "epoch 232, acc is 0.652542372881356\n",
            "epoch 233, loss is 0.5875692963600159\n",
            "epoch 233, acc is 0.652542372881356\n",
            "epoch 234, loss is 0.5875517725944519\n",
            "epoch 234, acc is 0.652542372881356\n",
            "epoch 235, loss is 0.5875341892242432\n",
            "epoch 235, acc is 0.652542372881356\n",
            "epoch 236, loss is 0.5875166654586792\n",
            "epoch 236, acc is 0.652542372881356\n",
            "epoch 237, loss is 0.58749920129776\n",
            "epoch 237, acc is 0.652542372881356\n",
            "epoch 238, loss is 0.5874817371368408\n",
            "epoch 238, acc is 0.652542372881356\n",
            "epoch 239, loss is 0.5874642729759216\n",
            "epoch 239, acc is 0.652542372881356\n",
            "epoch 240, loss is 0.5874468088150024\n",
            "epoch 240, acc is 0.652542372881356\n",
            "epoch 241, loss is 0.5874294638633728\n",
            "epoch 241, acc is 0.652542372881356\n",
            "epoch 242, loss is 0.5874120593070984\n",
            "epoch 242, acc is 0.652542372881356\n",
            "epoch 243, loss is 0.587394654750824\n",
            "epoch 243, acc is 0.652542372881356\n",
            "epoch 244, loss is 0.5873773097991943\n",
            "epoch 244, acc is 0.652542372881356\n",
            "epoch 245, loss is 0.5873600840568542\n",
            "epoch 245, acc is 0.652542372881356\n",
            "epoch 246, loss is 0.5873427391052246\n",
            "epoch 246, acc is 0.652542372881356\n",
            "epoch 247, loss is 0.5873255133628845\n",
            "epoch 247, acc is 0.652542372881356\n",
            "epoch 248, loss is 0.5873082280158997\n",
            "epoch 248, acc is 0.652542372881356\n",
            "epoch 249, loss is 0.5872910022735596\n",
            "epoch 249, acc is 0.652542372881356\n",
            "epoch 250, loss is 0.5872738361358643\n",
            "epoch 250, acc is 0.652542372881356\n",
            "epoch 251, loss is 0.587256669998169\n",
            "epoch 251, acc is 0.652542372881356\n",
            "epoch 252, loss is 0.5872395634651184\n",
            "epoch 252, acc is 0.652542372881356\n",
            "epoch 253, loss is 0.5872223973274231\n",
            "epoch 253, acc is 0.652542372881356\n",
            "epoch 254, loss is 0.5872052907943726\n",
            "epoch 254, acc is 0.652542372881356\n",
            "epoch 255, loss is 0.587188184261322\n",
            "epoch 255, acc is 0.652542372881356\n",
            "epoch 256, loss is 0.587171196937561\n",
            "epoch 256, acc is 0.652542372881356\n",
            "epoch 257, loss is 0.5871541500091553\n",
            "epoch 257, acc is 0.652542372881356\n",
            "epoch 258, loss is 0.5871371030807495\n",
            "epoch 258, acc is 0.652542372881356\n",
            "epoch 259, loss is 0.5871201753616333\n",
            "epoch 259, acc is 0.652542372881356\n",
            "epoch 260, loss is 0.5871031880378723\n",
            "epoch 260, acc is 0.652542372881356\n",
            "epoch 261, loss is 0.5870862603187561\n",
            "epoch 261, acc is 0.652542372881356\n",
            "epoch 262, loss is 0.5870692729949951\n",
            "epoch 262, acc is 0.652542372881356\n",
            "epoch 263, loss is 0.5870524048805237\n",
            "epoch 263, acc is 0.652542372881356\n",
            "epoch 264, loss is 0.5870354771614075\n",
            "epoch 264, acc is 0.652542372881356\n",
            "epoch 265, loss is 0.5870186686515808\n",
            "epoch 265, acc is 0.652542372881356\n",
            "epoch 266, loss is 0.5870019197463989\n",
            "epoch 266, acc is 0.652542372881356\n",
            "epoch 267, loss is 0.5869850516319275\n",
            "epoch 267, acc is 0.652542372881356\n",
            "epoch 268, loss is 0.5869682431221008\n",
            "epoch 268, acc is 0.652542372881356\n",
            "epoch 269, loss is 0.5869515538215637\n",
            "epoch 269, acc is 0.652542372881356\n",
            "epoch 270, loss is 0.5869347453117371\n",
            "epoch 270, acc is 0.652542372881356\n",
            "epoch 271, loss is 0.5869180560112\n",
            "epoch 271, acc is 0.652542372881356\n",
            "epoch 272, loss is 0.5869013667106628\n",
            "epoch 272, acc is 0.652542372881356\n",
            "epoch 273, loss is 0.5868847370147705\n",
            "epoch 273, acc is 0.652542372881356\n",
            "epoch 274, loss is 0.5868680477142334\n",
            "epoch 274, acc is 0.652542372881356\n",
            "epoch 275, loss is 0.5868514180183411\n",
            "epoch 275, acc is 0.652542372881356\n",
            "epoch 276, loss is 0.5868347883224487\n",
            "epoch 276, acc is 0.652542372881356\n",
            "epoch 277, loss is 0.5868182182312012\n",
            "epoch 277, acc is 0.652542372881356\n",
            "epoch 278, loss is 0.5868017077445984\n",
            "epoch 278, acc is 0.652542372881356\n",
            "epoch 279, loss is 0.5867851376533508\n",
            "epoch 279, acc is 0.652542372881356\n",
            "epoch 280, loss is 0.5867685675621033\n",
            "epoch 280, acc is 0.652542372881356\n",
            "epoch 281, loss is 0.5867521166801453\n",
            "epoch 281, acc is 0.652542372881356\n",
            "epoch 282, loss is 0.5867356061935425\n",
            "epoch 282, acc is 0.652542372881356\n",
            "epoch 283, loss is 0.5867192149162292\n",
            "epoch 283, acc is 0.652542372881356\n",
            "epoch 284, loss is 0.5867027640342712\n",
            "epoch 284, acc is 0.652542372881356\n",
            "epoch 285, loss is 0.586686372756958\n",
            "epoch 285, acc is 0.652542372881356\n",
            "epoch 286, loss is 0.586669921875\n",
            "epoch 286, acc is 0.652542372881356\n",
            "epoch 287, loss is 0.586653470993042\n",
            "epoch 287, acc is 0.652542372881356\n",
            "epoch 288, loss is 0.5866371989250183\n",
            "epoch 288, acc is 0.652542372881356\n",
            "epoch 289, loss is 0.5866209268569946\n",
            "epoch 289, acc is 0.652542372881356\n",
            "epoch 290, loss is 0.5866045951843262\n",
            "epoch 290, acc is 0.652542372881356\n",
            "epoch 291, loss is 0.5865883231163025\n",
            "epoch 291, acc is 0.652542372881356\n",
            "epoch 292, loss is 0.5865720510482788\n",
            "epoch 292, acc is 0.652542372881356\n",
            "epoch 293, loss is 0.5865557789802551\n",
            "epoch 293, acc is 0.652542372881356\n",
            "epoch 294, loss is 0.5865395665168762\n",
            "epoch 294, acc is 0.652542372881356\n",
            "epoch 295, loss is 0.5865233540534973\n",
            "epoch 295, acc is 0.652542372881356\n",
            "epoch 296, loss is 0.586507260799408\n",
            "epoch 296, acc is 0.652542372881356\n",
            "epoch 297, loss is 0.586491048336029\n",
            "epoch 297, acc is 0.652542372881356\n",
            "epoch 298, loss is 0.5864748954772949\n",
            "epoch 298, acc is 0.652542372881356\n",
            "epoch 299, loss is 0.5864587426185608\n",
            "epoch 299, acc is 0.652542372881356\n",
            "epoch 300, loss is 0.5864427089691162\n",
            "epoch 300, acc is 0.652542372881356\n",
            "epoch 301, loss is 0.5864266157150269\n",
            "epoch 301, acc is 0.652542372881356\n",
            "epoch 302, loss is 0.5864105224609375\n",
            "epoch 302, acc is 0.652542372881356\n",
            "epoch 303, loss is 0.5863944888114929\n",
            "epoch 303, acc is 0.652542372881356\n",
            "epoch 304, loss is 0.5863785743713379\n",
            "epoch 304, acc is 0.652542372881356\n",
            "epoch 305, loss is 0.5863625407218933\n",
            "epoch 305, acc is 0.652542372881356\n",
            "epoch 306, loss is 0.5863465070724487\n",
            "epoch 306, acc is 0.652542372881356\n",
            "epoch 307, loss is 0.5863305926322937\n",
            "epoch 307, acc is 0.652542372881356\n",
            "epoch 308, loss is 0.5863146781921387\n",
            "epoch 308, acc is 0.652542372881356\n",
            "epoch 309, loss is 0.5862988233566284\n",
            "epoch 309, acc is 0.652542372881356\n",
            "epoch 310, loss is 0.5862829089164734\n",
            "epoch 310, acc is 0.652542372881356\n",
            "epoch 311, loss is 0.5862669944763184\n",
            "epoch 311, acc is 0.652542372881356\n",
            "epoch 312, loss is 0.5862511992454529\n",
            "epoch 312, acc is 0.652542372881356\n",
            "epoch 313, loss is 0.5862352848052979\n",
            "epoch 313, acc is 0.652542372881356\n",
            "epoch 314, loss is 0.5862195491790771\n",
            "epoch 314, acc is 0.652542372881356\n",
            "epoch 315, loss is 0.5862037539482117\n",
            "epoch 315, acc is 0.652542372881356\n",
            "epoch 316, loss is 0.5861879587173462\n",
            "epoch 316, acc is 0.652542372881356\n",
            "epoch 317, loss is 0.5861722230911255\n",
            "epoch 317, acc is 0.652542372881356\n",
            "epoch 318, loss is 0.5861564874649048\n",
            "epoch 318, acc is 0.652542372881356\n",
            "epoch 319, loss is 0.5861407518386841\n",
            "epoch 319, acc is 0.652542372881356\n",
            "epoch 320, loss is 0.5861251354217529\n",
            "epoch 320, acc is 0.652542372881356\n",
            "epoch 321, loss is 0.5861093997955322\n",
            "epoch 321, acc is 0.652542372881356\n",
            "epoch 322, loss is 0.5860937833786011\n",
            "epoch 322, acc is 0.652542372881356\n",
            "epoch 323, loss is 0.5860781669616699\n",
            "epoch 323, acc is 0.652542372881356\n",
            "epoch 324, loss is 0.5860625505447388\n",
            "epoch 324, acc is 0.652542372881356\n",
            "epoch 325, loss is 0.5860468745231628\n",
            "epoch 325, acc is 0.652542372881356\n",
            "epoch 326, loss is 0.5860313773155212\n",
            "epoch 326, acc is 0.652542372881356\n",
            "epoch 327, loss is 0.5860158801078796\n",
            "epoch 327, acc is 0.652542372881356\n",
            "epoch 328, loss is 0.5860002636909485\n",
            "epoch 328, acc is 0.652542372881356\n",
            "epoch 329, loss is 0.5859847664833069\n",
            "epoch 329, acc is 0.652542372881356\n",
            "epoch 330, loss is 0.5859692692756653\n",
            "epoch 330, acc is 0.652542372881356\n",
            "epoch 331, loss is 0.5859537720680237\n",
            "epoch 331, acc is 0.652542372881356\n",
            "epoch 332, loss is 0.5859383940696716\n",
            "epoch 332, acc is 0.652542372881356\n",
            "epoch 333, loss is 0.58592289686203\n",
            "epoch 333, acc is 0.652542372881356\n",
            "epoch 334, loss is 0.585907518863678\n",
            "epoch 334, acc is 0.652542372881356\n",
            "epoch 335, loss is 0.5858921408653259\n",
            "epoch 335, acc is 0.652542372881356\n",
            "epoch 336, loss is 0.5858767032623291\n",
            "epoch 336, acc is 0.652542372881356\n",
            "epoch 337, loss is 0.585861325263977\n",
            "epoch 337, acc is 0.652542372881356\n",
            "epoch 338, loss is 0.5858460068702698\n",
            "epoch 338, acc is 0.652542372881356\n",
            "epoch 339, loss is 0.5858306884765625\n",
            "epoch 339, acc is 0.652542372881356\n",
            "epoch 340, loss is 0.5858154296875\n",
            "epoch 340, acc is 0.652542372881356\n",
            "epoch 341, loss is 0.5858001112937927\n",
            "epoch 341, acc is 0.652542372881356\n",
            "epoch 342, loss is 0.5857848525047302\n",
            "epoch 342, acc is 0.652542372881356\n",
            "epoch 343, loss is 0.5857696533203125\n",
            "epoch 343, acc is 0.652542372881356\n",
            "epoch 344, loss is 0.58575439453125\n",
            "epoch 344, acc is 0.652542372881356\n",
            "epoch 345, loss is 0.5857391357421875\n",
            "epoch 345, acc is 0.652542372881356\n",
            "epoch 346, loss is 0.5857239961624146\n",
            "epoch 346, acc is 0.652542372881356\n",
            "epoch 347, loss is 0.5857087969779968\n",
            "epoch 347, acc is 0.652542372881356\n",
            "epoch 348, loss is 0.5856936573982239\n",
            "epoch 348, acc is 0.652542372881356\n",
            "epoch 349, loss is 0.5856785178184509\n",
            "epoch 349, acc is 0.652542372881356\n",
            "epoch 350, loss is 0.5856634378433228\n",
            "epoch 350, acc is 0.652542372881356\n",
            "epoch 351, loss is 0.5856483578681946\n",
            "epoch 351, acc is 0.652542372881356\n",
            "epoch 352, loss is 0.5856332182884216\n",
            "epoch 352, acc is 0.652542372881356\n",
            "epoch 353, loss is 0.585618257522583\n",
            "epoch 353, acc is 0.652542372881356\n",
            "epoch 354, loss is 0.5856031179428101\n",
            "epoch 354, acc is 0.652542372881356\n",
            "epoch 355, loss is 0.5855881571769714\n",
            "epoch 355, acc is 0.652542372881356\n",
            "epoch 356, loss is 0.585573136806488\n",
            "epoch 356, acc is 0.652542372881356\n",
            "epoch 357, loss is 0.5855581164360046\n",
            "epoch 357, acc is 0.652542372881356\n",
            "epoch 358, loss is 0.5855432152748108\n",
            "epoch 358, acc is 0.652542372881356\n",
            "epoch 359, loss is 0.5855282545089722\n",
            "epoch 359, acc is 0.652542372881356\n",
            "epoch 360, loss is 0.5855132937431335\n",
            "epoch 360, acc is 0.652542372881356\n",
            "epoch 361, loss is 0.5854983329772949\n",
            "epoch 361, acc is 0.652542372881356\n",
            "epoch 362, loss is 0.5854834914207458\n",
            "epoch 362, acc is 0.652542372881356\n",
            "epoch 363, loss is 0.585468590259552\n",
            "epoch 363, acc is 0.652542372881356\n",
            "epoch 364, loss is 0.5854537487030029\n",
            "epoch 364, acc is 0.652542372881356\n",
            "epoch 365, loss is 0.5854389667510986\n",
            "epoch 365, acc is 0.652542372881356\n",
            "epoch 366, loss is 0.5854241251945496\n",
            "epoch 366, acc is 0.652542372881356\n",
            "epoch 367, loss is 0.5854092836380005\n",
            "epoch 367, acc is 0.652542372881356\n",
            "epoch 368, loss is 0.5853945016860962\n",
            "epoch 368, acc is 0.652542372881356\n",
            "epoch 369, loss is 0.5853797793388367\n",
            "epoch 369, acc is 0.652542372881356\n",
            "epoch 370, loss is 0.5853650569915771\n",
            "epoch 370, acc is 0.652542372881356\n",
            "epoch 371, loss is 0.5853502750396729\n",
            "epoch 371, acc is 0.652542372881356\n",
            "epoch 372, loss is 0.5853355526924133\n",
            "epoch 372, acc is 0.652542372881356\n",
            "epoch 373, loss is 0.5853208303451538\n",
            "epoch 373, acc is 0.652542372881356\n",
            "epoch 374, loss is 0.5853062272071838\n",
            "epoch 374, acc is 0.652542372881356\n",
            "epoch 375, loss is 0.5852915048599243\n",
            "epoch 375, acc is 0.652542372881356\n",
            "epoch 376, loss is 0.5852768421173096\n",
            "epoch 376, acc is 0.652542372881356\n",
            "epoch 377, loss is 0.5852622389793396\n",
            "epoch 377, acc is 0.652542372881356\n",
            "epoch 378, loss is 0.5852476358413696\n",
            "epoch 378, acc is 0.652542372881356\n",
            "epoch 379, loss is 0.5852330327033997\n",
            "epoch 379, acc is 0.652542372881356\n",
            "epoch 380, loss is 0.5852185487747192\n",
            "epoch 380, acc is 0.652542372881356\n",
            "epoch 381, loss is 0.5852038860321045\n",
            "epoch 381, acc is 0.652542372881356\n",
            "epoch 382, loss is 0.5851894617080688\n",
            "epoch 382, acc is 0.652542372881356\n",
            "epoch 383, loss is 0.5851748585700989\n",
            "epoch 383, acc is 0.652542372881356\n",
            "epoch 384, loss is 0.5851603150367737\n",
            "epoch 384, acc is 0.652542372881356\n",
            "epoch 385, loss is 0.5851458311080933\n",
            "epoch 385, acc is 0.652542372881356\n",
            "epoch 386, loss is 0.5851313471794128\n",
            "epoch 386, acc is 0.652542372881356\n",
            "epoch 387, loss is 0.585116982460022\n",
            "epoch 387, acc is 0.652542372881356\n",
            "epoch 388, loss is 0.5851025581359863\n",
            "epoch 388, acc is 0.652542372881356\n",
            "epoch 389, loss is 0.5850881338119507\n",
            "epoch 389, acc is 0.652542372881356\n",
            "epoch 390, loss is 0.585073709487915\n",
            "epoch 390, acc is 0.652542372881356\n",
            "epoch 391, loss is 0.5850593447685242\n",
            "epoch 391, acc is 0.652542372881356\n",
            "epoch 392, loss is 0.5850449800491333\n",
            "epoch 392, acc is 0.652542372881356\n",
            "epoch 393, loss is 0.5850305557250977\n",
            "epoch 393, acc is 0.652542372881356\n",
            "epoch 394, loss is 0.5850163102149963\n",
            "epoch 394, acc is 0.652542372881356\n",
            "epoch 395, loss is 0.5850019454956055\n",
            "epoch 395, acc is 0.652542372881356\n",
            "epoch 396, loss is 0.5849876999855042\n",
            "epoch 396, acc is 0.652542372881356\n",
            "epoch 397, loss is 0.5849733352661133\n",
            "epoch 397, acc is 0.652542372881356\n",
            "epoch 398, loss is 0.5849591493606567\n",
            "epoch 398, acc is 0.652542372881356\n",
            "epoch 399, loss is 0.5849448442459106\n",
            "epoch 399, acc is 0.652542372881356\n",
            "epoch 400, loss is 0.5849305987358093\n",
            "epoch 400, acc is 0.652542372881356\n",
            "epoch 401, loss is 0.5849164128303528\n",
            "epoch 401, acc is 0.652542372881356\n",
            "epoch 402, loss is 0.5849022269248962\n",
            "epoch 402, acc is 0.652542372881356\n",
            "epoch 403, loss is 0.5848879814147949\n",
            "epoch 403, acc is 0.652542372881356\n",
            "epoch 404, loss is 0.5848739147186279\n",
            "epoch 404, acc is 0.652542372881356\n",
            "epoch 405, loss is 0.5848596692085266\n",
            "epoch 405, acc is 0.652542372881356\n",
            "epoch 406, loss is 0.5848456025123596\n",
            "epoch 406, acc is 0.652542372881356\n",
            "epoch 407, loss is 0.5848314762115479\n",
            "epoch 407, acc is 0.652542372881356\n",
            "epoch 408, loss is 0.5848173499107361\n",
            "epoch 408, acc is 0.652542372881356\n",
            "epoch 409, loss is 0.5848032236099243\n",
            "epoch 409, acc is 0.652542372881356\n",
            "epoch 410, loss is 0.5847892165184021\n",
            "epoch 410, acc is 0.652542372881356\n",
            "epoch 411, loss is 0.5847750902175903\n",
            "epoch 411, acc is 0.652542372881356\n",
            "epoch 412, loss is 0.5847610831260681\n",
            "epoch 412, acc is 0.652542372881356\n",
            "epoch 413, loss is 0.5847471356391907\n",
            "epoch 413, acc is 0.652542372881356\n",
            "epoch 414, loss is 0.5847330689430237\n",
            "epoch 414, acc is 0.652542372881356\n",
            "epoch 415, loss is 0.5847191214561462\n",
            "epoch 415, acc is 0.652542372881356\n",
            "epoch 416, loss is 0.5847051739692688\n",
            "epoch 416, acc is 0.652542372881356\n",
            "epoch 417, loss is 0.5846911668777466\n",
            "epoch 417, acc is 0.652542372881356\n",
            "epoch 418, loss is 0.5846772193908691\n",
            "epoch 418, acc is 0.652542372881356\n",
            "epoch 419, loss is 0.5846633315086365\n",
            "epoch 419, acc is 0.652542372881356\n",
            "epoch 420, loss is 0.5846494436264038\n",
            "epoch 420, acc is 0.652542372881356\n",
            "epoch 421, loss is 0.5846354961395264\n",
            "epoch 421, acc is 0.652542372881356\n",
            "epoch 422, loss is 0.5846216678619385\n",
            "epoch 422, acc is 0.652542372881356\n",
            "epoch 423, loss is 0.5846077799797058\n",
            "epoch 423, acc is 0.652542372881356\n",
            "epoch 424, loss is 0.5845939517021179\n",
            "epoch 424, acc is 0.652542372881356\n",
            "epoch 425, loss is 0.58458012342453\n",
            "epoch 425, acc is 0.652542372881356\n",
            "epoch 426, loss is 0.5845662951469421\n",
            "epoch 426, acc is 0.652542372881356\n",
            "epoch 427, loss is 0.5845525860786438\n",
            "epoch 427, acc is 0.652542372881356\n",
            "epoch 428, loss is 0.5845387578010559\n",
            "epoch 428, acc is 0.652542372881356\n",
            "epoch 429, loss is 0.5845250487327576\n",
            "epoch 429, acc is 0.652542372881356\n",
            "epoch 430, loss is 0.5845112204551697\n",
            "epoch 430, acc is 0.652542372881356\n",
            "epoch 431, loss is 0.5844975113868713\n",
            "epoch 431, acc is 0.652542372881356\n",
            "epoch 432, loss is 0.584483802318573\n",
            "epoch 432, acc is 0.652542372881356\n",
            "epoch 433, loss is 0.5844700932502747\n",
            "epoch 433, acc is 0.652542372881356\n",
            "epoch 434, loss is 0.5844564437866211\n",
            "epoch 434, acc is 0.652542372881356\n",
            "epoch 435, loss is 0.5844427943229675\n",
            "epoch 435, acc is 0.652542372881356\n",
            "epoch 436, loss is 0.5844290852546692\n",
            "epoch 436, acc is 0.652542372881356\n",
            "epoch 437, loss is 0.5844154357910156\n",
            "epoch 437, acc is 0.652542372881356\n",
            "epoch 438, loss is 0.5844017863273621\n",
            "epoch 438, acc is 0.652542372881356\n",
            "epoch 439, loss is 0.5843881964683533\n",
            "epoch 439, acc is 0.652542372881356\n",
            "epoch 440, loss is 0.5843746662139893\n",
            "epoch 440, acc is 0.652542372881356\n",
            "epoch 441, loss is 0.5843610763549805\n",
            "epoch 441, acc is 0.652542372881356\n",
            "epoch 442, loss is 0.5843474864959717\n",
            "epoch 442, acc is 0.652542372881356\n",
            "epoch 443, loss is 0.5843338966369629\n",
            "epoch 443, acc is 0.652542372881356\n",
            "epoch 444, loss is 0.5843203663825989\n",
            "epoch 444, acc is 0.652542372881356\n",
            "epoch 445, loss is 0.5843068957328796\n",
            "epoch 445, acc is 0.652542372881356\n",
            "epoch 446, loss is 0.5842933654785156\n",
            "epoch 446, acc is 0.652542372881356\n",
            "epoch 447, loss is 0.5842798352241516\n",
            "epoch 447, acc is 0.652542372881356\n",
            "epoch 448, loss is 0.5842664241790771\n",
            "epoch 448, acc is 0.652542372881356\n",
            "epoch 449, loss is 0.5842529535293579\n",
            "epoch 449, acc is 0.652542372881356\n",
            "epoch 450, loss is 0.5842394828796387\n",
            "epoch 450, acc is 0.652542372881356\n",
            "epoch 451, loss is 0.5842260718345642\n",
            "epoch 451, acc is 0.652542372881356\n",
            "epoch 452, loss is 0.5842126607894897\n",
            "epoch 452, acc is 0.652542372881356\n",
            "epoch 453, loss is 0.5841993093490601\n",
            "epoch 453, acc is 0.652542372881356\n",
            "epoch 454, loss is 0.5841858983039856\n",
            "epoch 454, acc is 0.652542372881356\n",
            "epoch 455, loss is 0.5841725468635559\n",
            "epoch 455, acc is 0.652542372881356\n",
            "epoch 456, loss is 0.5841591954231262\n",
            "epoch 456, acc is 0.652542372881356\n",
            "epoch 457, loss is 0.5841459035873413\n",
            "epoch 457, acc is 0.652542372881356\n",
            "epoch 458, loss is 0.5841325521469116\n",
            "epoch 458, acc is 0.652542372881356\n",
            "epoch 459, loss is 0.5841192603111267\n",
            "epoch 459, acc is 0.652542372881356\n",
            "epoch 460, loss is 0.584105908870697\n",
            "epoch 460, acc is 0.652542372881356\n",
            "epoch 461, loss is 0.5840926170349121\n",
            "epoch 461, acc is 0.652542372881356\n",
            "epoch 462, loss is 0.5840794444084167\n",
            "epoch 462, acc is 0.652542372881356\n",
            "epoch 463, loss is 0.5840660929679871\n",
            "epoch 463, acc is 0.652542372881356\n",
            "epoch 464, loss is 0.5840528607368469\n",
            "epoch 464, acc is 0.652542372881356\n",
            "epoch 465, loss is 0.5840396285057068\n",
            "epoch 465, acc is 0.652542372881356\n",
            "epoch 466, loss is 0.5840264558792114\n",
            "epoch 466, acc is 0.652542372881356\n",
            "epoch 467, loss is 0.5840133428573608\n",
            "epoch 467, acc is 0.652542372881356\n",
            "epoch 468, loss is 0.5840001106262207\n",
            "epoch 468, acc is 0.652542372881356\n",
            "epoch 469, loss is 0.5839869379997253\n",
            "epoch 469, acc is 0.652542372881356\n",
            "epoch 470, loss is 0.58397376537323\n",
            "epoch 470, acc is 0.652542372881356\n",
            "epoch 471, loss is 0.5839606523513794\n",
            "epoch 471, acc is 0.652542372881356\n",
            "epoch 472, loss is 0.5839475393295288\n",
            "epoch 472, acc is 0.652542372881356\n",
            "epoch 473, loss is 0.583934485912323\n",
            "epoch 473, acc is 0.652542372881356\n",
            "epoch 474, loss is 0.5839213132858276\n",
            "epoch 474, acc is 0.652542372881356\n",
            "epoch 475, loss is 0.583908200263977\n",
            "epoch 475, acc is 0.652542372881356\n",
            "epoch 476, loss is 0.583895206451416\n",
            "epoch 476, acc is 0.652542372881356\n",
            "epoch 477, loss is 0.5838821530342102\n",
            "epoch 477, acc is 0.652542372881356\n",
            "epoch 478, loss is 0.5838691592216492\n",
            "epoch 478, acc is 0.652542372881356\n",
            "epoch 479, loss is 0.5838561058044434\n",
            "epoch 479, acc is 0.652542372881356\n",
            "epoch 480, loss is 0.5838431119918823\n",
            "epoch 480, acc is 0.652542372881356\n",
            "epoch 481, loss is 0.5838301181793213\n",
            "epoch 481, acc is 0.652542372881356\n",
            "epoch 482, loss is 0.583817183971405\n",
            "epoch 482, acc is 0.652542372881356\n",
            "epoch 483, loss is 0.5838041305541992\n",
            "epoch 483, acc is 0.652542372881356\n",
            "epoch 484, loss is 0.583791196346283\n",
            "epoch 484, acc is 0.652542372881356\n",
            "epoch 485, loss is 0.5837782621383667\n",
            "epoch 485, acc is 0.652542372881356\n",
            "epoch 486, loss is 0.5837653279304504\n",
            "epoch 486, acc is 0.652542372881356\n",
            "epoch 487, loss is 0.583752453327179\n",
            "epoch 487, acc is 0.652542372881356\n",
            "epoch 488, loss is 0.5837395787239075\n",
            "epoch 488, acc is 0.652542372881356\n",
            "epoch 489, loss is 0.5837266445159912\n",
            "epoch 489, acc is 0.652542372881356\n",
            "epoch 490, loss is 0.5837138295173645\n",
            "epoch 490, acc is 0.652542372881356\n",
            "epoch 491, loss is 0.5837008953094482\n",
            "epoch 491, acc is 0.652542372881356\n",
            "epoch 492, loss is 0.5836881399154663\n",
            "epoch 492, acc is 0.652542372881356\n",
            "epoch 493, loss is 0.5836753249168396\n",
            "epoch 493, acc is 0.652542372881356\n",
            "epoch 494, loss is 0.5836624503135681\n",
            "epoch 494, acc is 0.652542372881356\n",
            "epoch 495, loss is 0.5836496949195862\n",
            "epoch 495, acc is 0.652542372881356\n",
            "epoch 496, loss is 0.5836369395256042\n",
            "epoch 496, acc is 0.652542372881356\n",
            "epoch 497, loss is 0.5836241245269775\n",
            "epoch 497, acc is 0.652542372881356\n",
            "epoch 498, loss is 0.5836114287376404\n",
            "epoch 498, acc is 0.652542372881356\n",
            "epoch 499, loss is 0.5835986137390137\n",
            "epoch 499, acc is 0.652542372881356\n",
            "epoch 500, loss is 0.5835859179496765\n",
            "epoch 500, acc is 0.652542372881356\n",
            "epoch 501, loss is 0.5835731625556946\n",
            "epoch 501, acc is 0.652542372881356\n",
            "epoch 502, loss is 0.5835604667663574\n",
            "epoch 502, acc is 0.652542372881356\n",
            "epoch 503, loss is 0.583547830581665\n",
            "epoch 503, acc is 0.652542372881356\n",
            "epoch 504, loss is 0.5835351347923279\n",
            "epoch 504, acc is 0.652542372881356\n",
            "epoch 505, loss is 0.5835224986076355\n",
            "epoch 505, acc is 0.652542372881356\n",
            "epoch 506, loss is 0.5835098028182983\n",
            "epoch 506, acc is 0.652542372881356\n",
            "epoch 507, loss is 0.5834971070289612\n",
            "epoch 507, acc is 0.652542372881356\n",
            "epoch 508, loss is 0.5834845304489136\n",
            "epoch 508, acc is 0.652542372881356\n",
            "epoch 509, loss is 0.5834718942642212\n",
            "epoch 509, acc is 0.652542372881356\n",
            "epoch 510, loss is 0.5834593176841736\n",
            "epoch 510, acc is 0.652542372881356\n",
            "epoch 511, loss is 0.583446741104126\n",
            "epoch 511, acc is 0.652542372881356\n",
            "epoch 512, loss is 0.5834341645240784\n",
            "epoch 512, acc is 0.652542372881356\n",
            "epoch 513, loss is 0.5834215879440308\n",
            "epoch 513, acc is 0.652542372881356\n",
            "epoch 514, loss is 0.5834090709686279\n",
            "epoch 514, acc is 0.652542372881356\n",
            "epoch 515, loss is 0.5833965539932251\n",
            "epoch 515, acc is 0.652542372881356\n",
            "epoch 516, loss is 0.5833840370178223\n",
            "epoch 516, acc is 0.652542372881356\n",
            "epoch 517, loss is 0.5833715796470642\n",
            "epoch 517, acc is 0.652542372881356\n",
            "epoch 518, loss is 0.5833590030670166\n",
            "epoch 518, acc is 0.652542372881356\n",
            "epoch 519, loss is 0.5833466053009033\n",
            "epoch 519, acc is 0.652542372881356\n",
            "epoch 520, loss is 0.5833341479301453\n",
            "epoch 520, acc is 0.652542372881356\n",
            "epoch 521, loss is 0.5833216309547424\n",
            "epoch 521, acc is 0.652542372881356\n",
            "epoch 522, loss is 0.5833092331886292\n",
            "epoch 522, acc is 0.652542372881356\n",
            "epoch 523, loss is 0.5832968354225159\n",
            "epoch 523, acc is 0.652542372881356\n",
            "epoch 524, loss is 0.5832844376564026\n",
            "epoch 524, acc is 0.652542372881356\n",
            "epoch 525, loss is 0.5832719802856445\n",
            "epoch 525, acc is 0.652542372881356\n",
            "epoch 526, loss is 0.5832595825195312\n",
            "epoch 526, acc is 0.652542372881356\n",
            "epoch 527, loss is 0.5832472443580627\n",
            "epoch 527, acc is 0.652542372881356\n",
            "epoch 528, loss is 0.5832349061965942\n",
            "epoch 528, acc is 0.652542372881356\n",
            "epoch 529, loss is 0.5832224488258362\n",
            "epoch 529, acc is 0.652542372881356\n",
            "epoch 530, loss is 0.5832101702690125\n",
            "epoch 530, acc is 0.652542372881356\n",
            "epoch 531, loss is 0.583197832107544\n",
            "epoch 531, acc is 0.652542372881356\n",
            "epoch 532, loss is 0.5831855535507202\n",
            "epoch 532, acc is 0.652542372881356\n",
            "epoch 533, loss is 0.5831732153892517\n",
            "epoch 533, acc is 0.652542372881356\n",
            "epoch 534, loss is 0.583160936832428\n",
            "epoch 534, acc is 0.652542372881356\n",
            "epoch 535, loss is 0.5831486582756042\n",
            "epoch 535, acc is 0.652542372881356\n",
            "epoch 536, loss is 0.5831364393234253\n",
            "epoch 536, acc is 0.652542372881356\n",
            "epoch 537, loss is 0.5831241011619568\n",
            "epoch 537, acc is 0.652542372881356\n",
            "epoch 538, loss is 0.5831118822097778\n",
            "epoch 538, acc is 0.652542372881356\n",
            "epoch 539, loss is 0.5830996632575989\n",
            "epoch 539, acc is 0.652542372881356\n",
            "epoch 540, loss is 0.5830874443054199\n",
            "epoch 540, acc is 0.652542372881356\n",
            "epoch 541, loss is 0.5830752849578857\n",
            "epoch 541, acc is 0.652542372881356\n",
            "epoch 542, loss is 0.5830631852149963\n",
            "epoch 542, acc is 0.652542372881356\n",
            "epoch 543, loss is 0.5830509662628174\n",
            "epoch 543, acc is 0.652542372881356\n",
            "epoch 544, loss is 0.5830388069152832\n",
            "epoch 544, acc is 0.652542372881356\n",
            "epoch 545, loss is 0.583026647567749\n",
            "epoch 545, acc is 0.652542372881356\n",
            "epoch 546, loss is 0.5830145478248596\n",
            "epoch 546, acc is 0.652542372881356\n",
            "epoch 547, loss is 0.5830023884773254\n",
            "epoch 547, acc is 0.652542372881356\n",
            "epoch 548, loss is 0.5829902291297913\n",
            "epoch 548, acc is 0.652542372881356\n",
            "epoch 549, loss is 0.5829781293869019\n",
            "epoch 549, acc is 0.652542372881356\n",
            "epoch 550, loss is 0.5829660296440125\n",
            "epoch 550, acc is 0.652542372881356\n",
            "epoch 551, loss is 0.582953929901123\n",
            "epoch 551, acc is 0.652542372881356\n",
            "epoch 552, loss is 0.582942008972168\n",
            "epoch 552, acc is 0.652542372881356\n",
            "epoch 553, loss is 0.5829298496246338\n",
            "epoch 553, acc is 0.652542372881356\n",
            "epoch 554, loss is 0.5829178690910339\n",
            "epoch 554, acc is 0.652542372881356\n",
            "epoch 555, loss is 0.5829057693481445\n",
            "epoch 555, acc is 0.652542372881356\n",
            "epoch 556, loss is 0.5828937888145447\n",
            "epoch 556, acc is 0.652542372881356\n",
            "epoch 557, loss is 0.5828818082809448\n",
            "epoch 557, acc is 0.652542372881356\n",
            "epoch 558, loss is 0.582869827747345\n",
            "epoch 558, acc is 0.652542372881356\n",
            "epoch 559, loss is 0.5828578472137451\n",
            "epoch 559, acc is 0.652542372881356\n",
            "epoch 560, loss is 0.5828458070755005\n",
            "epoch 560, acc is 0.652542372881356\n",
            "epoch 561, loss is 0.5828338861465454\n",
            "epoch 561, acc is 0.652542372881356\n",
            "epoch 562, loss is 0.5828219056129456\n",
            "epoch 562, acc is 0.652542372881356\n",
            "epoch 563, loss is 0.5828100442886353\n",
            "epoch 563, acc is 0.652542372881356\n",
            "epoch 564, loss is 0.5827980637550354\n",
            "epoch 564, acc is 0.652542372881356\n",
            "epoch 565, loss is 0.5827862024307251\n",
            "epoch 565, acc is 0.652542372881356\n",
            "epoch 566, loss is 0.58277428150177\n",
            "epoch 566, acc is 0.652542372881356\n",
            "epoch 567, loss is 0.5827623605728149\n",
            "epoch 567, acc is 0.652542372881356\n",
            "epoch 568, loss is 0.5827504992485046\n",
            "epoch 568, acc is 0.652542372881356\n",
            "epoch 569, loss is 0.5827386379241943\n",
            "epoch 569, acc is 0.652542372881356\n",
            "epoch 570, loss is 0.5827268362045288\n",
            "epoch 570, acc is 0.652542372881356\n",
            "epoch 571, loss is 0.5827149748802185\n",
            "epoch 571, acc is 0.652542372881356\n",
            "epoch 572, loss is 0.582703173160553\n",
            "epoch 572, acc is 0.652542372881356\n",
            "epoch 573, loss is 0.5826913714408875\n",
            "epoch 573, acc is 0.652542372881356\n",
            "epoch 574, loss is 0.5826795697212219\n",
            "epoch 574, acc is 0.652542372881356\n",
            "epoch 575, loss is 0.5826677083969116\n",
            "epoch 575, acc is 0.652542372881356\n",
            "epoch 576, loss is 0.5826559662818909\n",
            "epoch 576, acc is 0.652542372881356\n",
            "epoch 577, loss is 0.5826441645622253\n",
            "epoch 577, acc is 0.652542372881356\n",
            "epoch 578, loss is 0.5826324224472046\n",
            "epoch 578, acc is 0.652542372881356\n",
            "epoch 579, loss is 0.5826207995414734\n",
            "epoch 579, acc is 0.652542372881356\n",
            "epoch 580, loss is 0.5826089978218079\n",
            "epoch 580, acc is 0.652542372881356\n",
            "epoch 581, loss is 0.5825972557067871\n",
            "epoch 581, acc is 0.652542372881356\n",
            "epoch 582, loss is 0.5825855731964111\n",
            "epoch 582, acc is 0.652542372881356\n",
            "epoch 583, loss is 0.5825738310813904\n",
            "epoch 583, acc is 0.652542372881356\n",
            "epoch 584, loss is 0.5825622081756592\n",
            "epoch 584, acc is 0.652542372881356\n",
            "epoch 585, loss is 0.5825505256652832\n",
            "epoch 585, acc is 0.652542372881356\n",
            "epoch 586, loss is 0.5825387835502625\n",
            "epoch 586, acc is 0.652542372881356\n",
            "epoch 587, loss is 0.5825271606445312\n",
            "epoch 587, acc is 0.652542372881356\n",
            "epoch 588, loss is 0.5825155973434448\n",
            "epoch 588, acc is 0.652542372881356\n",
            "epoch 589, loss is 0.5825039148330688\n",
            "epoch 589, acc is 0.652542372881356\n",
            "epoch 590, loss is 0.5824922323226929\n",
            "epoch 590, acc is 0.652542372881356\n",
            "epoch 591, loss is 0.5824806690216064\n",
            "epoch 591, acc is 0.652542372881356\n",
            "epoch 592, loss is 0.58246910572052\n",
            "epoch 592, acc is 0.652542372881356\n",
            "epoch 593, loss is 0.5824574828147888\n",
            "epoch 593, acc is 0.652542372881356\n",
            "epoch 594, loss is 0.5824459791183472\n",
            "epoch 594, acc is 0.652542372881356\n",
            "epoch 595, loss is 0.582434356212616\n",
            "epoch 595, acc is 0.652542372881356\n",
            "epoch 596, loss is 0.5824227929115295\n",
            "epoch 596, acc is 0.652542372881356\n",
            "epoch 597, loss is 0.5824112892150879\n",
            "epoch 597, acc is 0.652542372881356\n",
            "epoch 598, loss is 0.5823997259140015\n",
            "epoch 598, acc is 0.652542372881356\n",
            "epoch 599, loss is 0.5823882222175598\n",
            "epoch 599, acc is 0.652542372881356\n",
            "epoch 600, loss is 0.5823767185211182\n",
            "epoch 600, acc is 0.652542372881356\n",
            "epoch 601, loss is 0.5823652744293213\n",
            "epoch 601, acc is 0.652542372881356\n",
            "epoch 602, loss is 0.5823537707328796\n",
            "epoch 602, acc is 0.652542372881356\n",
            "epoch 603, loss is 0.582342267036438\n",
            "epoch 603, acc is 0.652542372881356\n",
            "epoch 604, loss is 0.5823307633399963\n",
            "epoch 604, acc is 0.652542372881356\n",
            "epoch 605, loss is 0.5823193788528442\n",
            "epoch 605, acc is 0.652542372881356\n",
            "epoch 606, loss is 0.5823078751564026\n",
            "epoch 606, acc is 0.652542372881356\n",
            "epoch 607, loss is 0.5822964906692505\n",
            "epoch 607, acc is 0.652542372881356\n",
            "epoch 608, loss is 0.5822850465774536\n",
            "epoch 608, acc is 0.652542372881356\n",
            "epoch 609, loss is 0.5822736024856567\n",
            "epoch 609, acc is 0.652542372881356\n",
            "epoch 610, loss is 0.5822622179985046\n",
            "epoch 610, acc is 0.652542372881356\n",
            "epoch 611, loss is 0.5822508931159973\n",
            "epoch 611, acc is 0.652542372881356\n",
            "epoch 612, loss is 0.5822394490242004\n",
            "epoch 612, acc is 0.652542372881356\n",
            "epoch 613, loss is 0.5822281241416931\n",
            "epoch 613, acc is 0.652542372881356\n",
            "epoch 614, loss is 0.582216739654541\n",
            "epoch 614, acc is 0.652542372881356\n",
            "epoch 615, loss is 0.5822054147720337\n",
            "epoch 615, acc is 0.652542372881356\n",
            "epoch 616, loss is 0.5821940302848816\n",
            "epoch 616, acc is 0.652542372881356\n",
            "epoch 617, loss is 0.582182765007019\n",
            "epoch 617, acc is 0.652542372881356\n",
            "epoch 618, loss is 0.5821714401245117\n",
            "epoch 618, acc is 0.652542372881356\n",
            "epoch 619, loss is 0.5821601152420044\n",
            "epoch 619, acc is 0.652542372881356\n",
            "epoch 620, loss is 0.5821487903594971\n",
            "epoch 620, acc is 0.652542372881356\n",
            "epoch 621, loss is 0.5821375250816345\n",
            "epoch 621, acc is 0.652542372881356\n",
            "epoch 622, loss is 0.5821263194084167\n",
            "epoch 622, acc is 0.652542372881356\n",
            "epoch 623, loss is 0.5821150541305542\n",
            "epoch 623, acc is 0.652542372881356\n",
            "epoch 624, loss is 0.5821037888526917\n",
            "epoch 624, acc is 0.652542372881356\n",
            "epoch 625, loss is 0.5820925235748291\n",
            "epoch 625, acc is 0.652542372881356\n",
            "epoch 626, loss is 0.5820813179016113\n",
            "epoch 626, acc is 0.652542372881356\n",
            "epoch 627, loss is 0.5820701122283936\n",
            "epoch 627, acc is 0.652542372881356\n",
            "epoch 628, loss is 0.582058846950531\n",
            "epoch 628, acc is 0.652542372881356\n",
            "epoch 629, loss is 0.582047700881958\n",
            "epoch 629, acc is 0.652542372881356\n",
            "epoch 630, loss is 0.5820364952087402\n",
            "epoch 630, acc is 0.652542372881356\n",
            "epoch 631, loss is 0.5820252895355225\n",
            "epoch 631, acc is 0.652542372881356\n",
            "epoch 632, loss is 0.5820141434669495\n",
            "epoch 632, acc is 0.652542372881356\n",
            "epoch 633, loss is 0.5820029973983765\n",
            "epoch 633, acc is 0.652542372881356\n",
            "epoch 634, loss is 0.5819918513298035\n",
            "epoch 634, acc is 0.652542372881356\n",
            "epoch 635, loss is 0.5819807052612305\n",
            "epoch 635, acc is 0.652542372881356\n",
            "epoch 636, loss is 0.5819695591926575\n",
            "epoch 636, acc is 0.652542372881356\n",
            "epoch 637, loss is 0.5819584727287292\n",
            "epoch 637, acc is 0.652542372881356\n",
            "epoch 638, loss is 0.581947386264801\n",
            "epoch 638, acc is 0.652542372881356\n",
            "epoch 639, loss is 0.5819362998008728\n",
            "epoch 639, acc is 0.652542372881356\n",
            "epoch 640, loss is 0.5819251537322998\n",
            "epoch 640, acc is 0.652542372881356\n",
            "epoch 641, loss is 0.5819140672683716\n",
            "epoch 641, acc is 0.652542372881356\n",
            "epoch 642, loss is 0.5819031000137329\n",
            "epoch 642, acc is 0.652542372881356\n",
            "epoch 643, loss is 0.5818920731544495\n",
            "epoch 643, acc is 0.652542372881356\n",
            "epoch 644, loss is 0.5818809866905212\n",
            "epoch 644, acc is 0.652542372881356\n",
            "epoch 645, loss is 0.5818699598312378\n",
            "epoch 645, acc is 0.652542372881356\n",
            "epoch 646, loss is 0.5818588733673096\n",
            "epoch 646, acc is 0.652542372881356\n",
            "epoch 647, loss is 0.5818479061126709\n",
            "epoch 647, acc is 0.652542372881356\n",
            "epoch 648, loss is 0.5818368792533875\n",
            "epoch 648, acc is 0.652542372881356\n",
            "epoch 649, loss is 0.5818259119987488\n",
            "epoch 649, acc is 0.652542372881356\n",
            "epoch 650, loss is 0.5818148851394653\n",
            "epoch 650, acc is 0.652542372881356\n",
            "epoch 651, loss is 0.5818039774894714\n",
            "epoch 651, acc is 0.652542372881356\n",
            "epoch 652, loss is 0.581792950630188\n",
            "epoch 652, acc is 0.652542372881356\n",
            "epoch 653, loss is 0.5817820429801941\n",
            "epoch 653, acc is 0.652542372881356\n",
            "epoch 654, loss is 0.5817710757255554\n",
            "epoch 654, acc is 0.652542372881356\n",
            "epoch 655, loss is 0.5817601084709167\n",
            "epoch 655, acc is 0.652542372881356\n",
            "epoch 656, loss is 0.5817492008209229\n",
            "epoch 656, acc is 0.652542372881356\n",
            "epoch 657, loss is 0.5817383527755737\n",
            "epoch 657, acc is 0.652542372881356\n",
            "epoch 658, loss is 0.5817274451255798\n",
            "epoch 658, acc is 0.652542372881356\n",
            "epoch 659, loss is 0.5817165970802307\n",
            "epoch 659, acc is 0.652542372881356\n",
            "epoch 660, loss is 0.581705629825592\n",
            "epoch 660, acc is 0.652542372881356\n",
            "epoch 661, loss is 0.5816947817802429\n",
            "epoch 661, acc is 0.652542372881356\n",
            "epoch 662, loss is 0.5816839337348938\n",
            "epoch 662, acc is 0.652542372881356\n",
            "epoch 663, loss is 0.5816730856895447\n",
            "epoch 663, acc is 0.652542372881356\n",
            "epoch 664, loss is 0.5816621780395508\n",
            "epoch 664, acc is 0.652542372881356\n",
            "epoch 665, loss is 0.5816513895988464\n",
            "epoch 665, acc is 0.652542372881356\n",
            "epoch 666, loss is 0.5816405415534973\n",
            "epoch 666, acc is 0.652542372881356\n",
            "epoch 667, loss is 0.581629753112793\n",
            "epoch 667, acc is 0.652542372881356\n",
            "epoch 668, loss is 0.5816189050674438\n",
            "epoch 668, acc is 0.652542372881356\n",
            "epoch 669, loss is 0.5816081166267395\n",
            "epoch 669, acc is 0.652542372881356\n",
            "epoch 670, loss is 0.5815973281860352\n",
            "epoch 670, acc is 0.652542372881356\n",
            "epoch 671, loss is 0.5815865397453308\n",
            "epoch 671, acc is 0.652542372881356\n",
            "epoch 672, loss is 0.5815758109092712\n",
            "epoch 672, acc is 0.652542372881356\n",
            "epoch 673, loss is 0.5815650820732117\n",
            "epoch 673, acc is 0.652542372881356\n",
            "epoch 674, loss is 0.5815542936325073\n",
            "epoch 674, acc is 0.652542372881356\n",
            "epoch 675, loss is 0.5815436244010925\n",
            "epoch 675, acc is 0.652542372881356\n",
            "epoch 676, loss is 0.5815328359603882\n",
            "epoch 676, acc is 0.652542372881356\n",
            "epoch 677, loss is 0.5815221667289734\n",
            "epoch 677, acc is 0.652542372881356\n",
            "epoch 678, loss is 0.5815114378929138\n",
            "epoch 678, acc is 0.652542372881356\n",
            "epoch 679, loss is 0.5815007090568542\n",
            "epoch 679, acc is 0.652542372881356\n",
            "epoch 680, loss is 0.5814899802207947\n",
            "epoch 680, acc is 0.652542372881356\n",
            "epoch 681, loss is 0.5814793705940247\n",
            "epoch 681, acc is 0.652542372881356\n",
            "epoch 682, loss is 0.5814686417579651\n",
            "epoch 682, acc is 0.652542372881356\n",
            "epoch 683, loss is 0.5814580321311951\n",
            "epoch 683, acc is 0.652542372881356\n",
            "epoch 684, loss is 0.5814473032951355\n",
            "epoch 684, acc is 0.652542372881356\n",
            "epoch 685, loss is 0.5814366936683655\n",
            "epoch 685, acc is 0.652542372881356\n",
            "epoch 686, loss is 0.5814260840415955\n",
            "epoch 686, acc is 0.652542372881356\n",
            "epoch 687, loss is 0.5814154148101807\n",
            "epoch 687, acc is 0.652542372881356\n",
            "epoch 688, loss is 0.5814048051834106\n",
            "epoch 688, acc is 0.652542372881356\n",
            "epoch 689, loss is 0.5813942551612854\n",
            "epoch 689, acc is 0.652542372881356\n",
            "epoch 690, loss is 0.5813835263252258\n",
            "epoch 690, acc is 0.652542372881356\n",
            "epoch 691, loss is 0.5813729166984558\n",
            "epoch 691, acc is 0.652542372881356\n",
            "epoch 692, loss is 0.5813624262809753\n",
            "epoch 692, acc is 0.652542372881356\n",
            "epoch 693, loss is 0.5813518762588501\n",
            "epoch 693, acc is 0.652542372881356\n",
            "epoch 694, loss is 0.5813412666320801\n",
            "epoch 694, acc is 0.652542372881356\n",
            "epoch 695, loss is 0.5813307762145996\n",
            "epoch 695, acc is 0.652542372881356\n",
            "epoch 696, loss is 0.5813201665878296\n",
            "epoch 696, acc is 0.652542372881356\n",
            "epoch 697, loss is 0.5813096761703491\n",
            "epoch 697, acc is 0.652542372881356\n",
            "epoch 698, loss is 0.5812990665435791\n",
            "epoch 698, acc is 0.652542372881356\n",
            "epoch 699, loss is 0.5812886357307434\n",
            "epoch 699, acc is 0.652542372881356\n",
            "epoch 700, loss is 0.5812781453132629\n",
            "epoch 700, acc is 0.652542372881356\n",
            "epoch 701, loss is 0.5812675356864929\n",
            "epoch 701, acc is 0.652542372881356\n",
            "epoch 702, loss is 0.5812570452690125\n",
            "epoch 702, acc is 0.652542372881356\n",
            "epoch 703, loss is 0.5812466740608215\n",
            "epoch 703, acc is 0.652542372881356\n",
            "epoch 704, loss is 0.5812361836433411\n",
            "epoch 704, acc is 0.652542372881356\n",
            "epoch 705, loss is 0.5812256932258606\n",
            "epoch 705, acc is 0.652542372881356\n",
            "epoch 706, loss is 0.5812153220176697\n",
            "epoch 706, acc is 0.652542372881356\n",
            "epoch 707, loss is 0.5812048316001892\n",
            "epoch 707, acc is 0.652542372881356\n",
            "epoch 708, loss is 0.5811943411827087\n",
            "epoch 708, acc is 0.652542372881356\n",
            "epoch 709, loss is 0.5811840295791626\n",
            "epoch 709, acc is 0.652542372881356\n",
            "epoch 710, loss is 0.5811735391616821\n",
            "epoch 710, acc is 0.652542372881356\n",
            "epoch 711, loss is 0.5811631083488464\n",
            "epoch 711, acc is 0.652542372881356\n",
            "epoch 712, loss is 0.5811526775360107\n",
            "epoch 712, acc is 0.652542372881356\n",
            "epoch 713, loss is 0.5811423659324646\n",
            "epoch 713, acc is 0.652542372881356\n",
            "epoch 714, loss is 0.5811319351196289\n",
            "epoch 714, acc is 0.652542372881356\n",
            "epoch 715, loss is 0.581121563911438\n",
            "epoch 715, acc is 0.652542372881356\n",
            "epoch 716, loss is 0.5811111927032471\n",
            "epoch 716, acc is 0.652542372881356\n",
            "epoch 717, loss is 0.5811008810997009\n",
            "epoch 717, acc is 0.652542372881356\n",
            "epoch 718, loss is 0.5810904502868652\n",
            "epoch 718, acc is 0.652542372881356\n",
            "epoch 719, loss is 0.5810801386833191\n",
            "epoch 719, acc is 0.652542372881356\n",
            "epoch 720, loss is 0.581069827079773\n",
            "epoch 720, acc is 0.652542372881356\n",
            "epoch 721, loss is 0.5810595154762268\n",
            "epoch 721, acc is 0.652542372881356\n",
            "epoch 722, loss is 0.5810492634773254\n",
            "epoch 722, acc is 0.652542372881356\n",
            "epoch 723, loss is 0.5810388922691345\n",
            "epoch 723, acc is 0.652542372881356\n",
            "epoch 724, loss is 0.5810285806655884\n",
            "epoch 724, acc is 0.652542372881356\n",
            "epoch 725, loss is 0.581018328666687\n",
            "epoch 725, acc is 0.652542372881356\n",
            "epoch 726, loss is 0.5810080170631409\n",
            "epoch 726, acc is 0.652542372881356\n",
            "epoch 727, loss is 0.5809977650642395\n",
            "epoch 727, acc is 0.652542372881356\n",
            "epoch 728, loss is 0.5809875130653381\n",
            "epoch 728, acc is 0.652542372881356\n",
            "epoch 729, loss is 0.5809773206710815\n",
            "epoch 729, acc is 0.652542372881356\n",
            "epoch 730, loss is 0.5809670090675354\n",
            "epoch 730, acc is 0.652542372881356\n",
            "epoch 731, loss is 0.5809568166732788\n",
            "epoch 731, acc is 0.652542372881356\n",
            "epoch 732, loss is 0.5809465646743774\n",
            "epoch 732, acc is 0.652542372881356\n",
            "epoch 733, loss is 0.5809363126754761\n",
            "epoch 733, acc is 0.652542372881356\n",
            "epoch 734, loss is 0.5809261798858643\n",
            "epoch 734, acc is 0.652542372881356\n",
            "epoch 735, loss is 0.5809159278869629\n",
            "epoch 735, acc is 0.652542372881356\n",
            "epoch 736, loss is 0.5809057354927063\n",
            "epoch 736, acc is 0.652542372881356\n",
            "epoch 737, loss is 0.5808954834938049\n",
            "epoch 737, acc is 0.652542372881356\n",
            "epoch 738, loss is 0.5808854103088379\n",
            "epoch 738, acc is 0.652542372881356\n",
            "epoch 739, loss is 0.5808752179145813\n",
            "epoch 739, acc is 0.652542372881356\n",
            "epoch 740, loss is 0.5808650255203247\n",
            "epoch 740, acc is 0.652542372881356\n",
            "epoch 741, loss is 0.5808548927307129\n",
            "epoch 741, acc is 0.652542372881356\n",
            "epoch 742, loss is 0.5808448195457458\n",
            "epoch 742, acc is 0.652542372881356\n",
            "epoch 743, loss is 0.580834686756134\n",
            "epoch 743, acc is 0.652542372881356\n",
            "epoch 744, loss is 0.5808244943618774\n",
            "epoch 744, acc is 0.652542372881356\n",
            "epoch 745, loss is 0.5808143615722656\n",
            "epoch 745, acc is 0.652542372881356\n",
            "epoch 746, loss is 0.5808043479919434\n",
            "epoch 746, acc is 0.652542372881356\n",
            "epoch 747, loss is 0.5807942152023315\n",
            "epoch 747, acc is 0.652542372881356\n",
            "epoch 748, loss is 0.5807840824127197\n",
            "epoch 748, acc is 0.652542372881356\n",
            "epoch 749, loss is 0.5807740688323975\n",
            "epoch 749, acc is 0.652542372881356\n",
            "epoch 750, loss is 0.5807639360427856\n",
            "epoch 750, acc is 0.652542372881356\n",
            "epoch 751, loss is 0.5807539224624634\n",
            "epoch 751, acc is 0.652542372881356\n",
            "epoch 752, loss is 0.5807438492774963\n",
            "epoch 752, acc is 0.652542372881356\n",
            "epoch 753, loss is 0.5807337164878845\n",
            "epoch 753, acc is 0.652542372881356\n",
            "epoch 754, loss is 0.5807237029075623\n",
            "epoch 754, acc is 0.652542372881356\n",
            "epoch 755, loss is 0.58071368932724\n",
            "epoch 755, acc is 0.652542372881356\n",
            "epoch 756, loss is 0.5807036757469177\n",
            "epoch 756, acc is 0.652542372881356\n",
            "epoch 757, loss is 0.5806936621665955\n",
            "epoch 757, acc is 0.652542372881356\n",
            "epoch 758, loss is 0.5806836485862732\n",
            "epoch 758, acc is 0.652542372881356\n",
            "epoch 759, loss is 0.5806736350059509\n",
            "epoch 759, acc is 0.652542372881356\n",
            "epoch 760, loss is 0.5806636214256287\n",
            "epoch 760, acc is 0.652542372881356\n",
            "epoch 761, loss is 0.5806536674499512\n",
            "epoch 761, acc is 0.652542372881356\n",
            "epoch 762, loss is 0.5806436538696289\n",
            "epoch 762, acc is 0.652542372881356\n",
            "epoch 763, loss is 0.5806336998939514\n",
            "epoch 763, acc is 0.652542372881356\n",
            "epoch 764, loss is 0.5806237459182739\n",
            "epoch 764, acc is 0.652542372881356\n",
            "epoch 765, loss is 0.5806137919425964\n",
            "epoch 765, acc is 0.652542372881356\n",
            "epoch 766, loss is 0.580603837966919\n",
            "epoch 766, acc is 0.652542372881356\n",
            "epoch 767, loss is 0.5805938839912415\n",
            "epoch 767, acc is 0.652542372881356\n",
            "epoch 768, loss is 0.5805839896202087\n",
            "epoch 768, acc is 0.652542372881356\n",
            "epoch 769, loss is 0.580574095249176\n",
            "epoch 769, acc is 0.652542372881356\n",
            "epoch 770, loss is 0.5805641412734985\n",
            "epoch 770, acc is 0.652542372881356\n",
            "epoch 771, loss is 0.5805542469024658\n",
            "epoch 771, acc is 0.652542372881356\n",
            "epoch 772, loss is 0.5805443525314331\n",
            "epoch 772, acc is 0.652542372881356\n",
            "epoch 773, loss is 0.5805344581604004\n",
            "epoch 773, acc is 0.652542372881356\n",
            "epoch 774, loss is 0.5805245637893677\n",
            "epoch 774, acc is 0.652542372881356\n",
            "epoch 775, loss is 0.5805147290229797\n",
            "epoch 775, acc is 0.652542372881356\n",
            "epoch 776, loss is 0.580504834651947\n",
            "epoch 776, acc is 0.652542372881356\n",
            "epoch 777, loss is 0.5804949998855591\n",
            "epoch 777, acc is 0.652542372881356\n",
            "epoch 778, loss is 0.5804850459098816\n",
            "epoch 778, acc is 0.652542372881356\n",
            "epoch 779, loss is 0.5804752707481384\n",
            "epoch 779, acc is 0.652542372881356\n",
            "epoch 780, loss is 0.5804654359817505\n",
            "epoch 780, acc is 0.652542372881356\n",
            "epoch 781, loss is 0.5804556012153625\n",
            "epoch 781, acc is 0.652542372881356\n",
            "epoch 782, loss is 0.5804457664489746\n",
            "epoch 782, acc is 0.652542372881356\n",
            "epoch 783, loss is 0.5804359316825867\n",
            "epoch 783, acc is 0.652542372881356\n",
            "epoch 784, loss is 0.5804261565208435\n",
            "epoch 784, acc is 0.652542372881356\n",
            "epoch 785, loss is 0.5804163813591003\n",
            "epoch 785, acc is 0.652542372881356\n",
            "epoch 786, loss is 0.5804066061973572\n",
            "epoch 786, acc is 0.652542372881356\n",
            "epoch 787, loss is 0.580396831035614\n",
            "epoch 787, acc is 0.652542372881356\n",
            "epoch 788, loss is 0.5803870558738708\n",
            "epoch 788, acc is 0.652542372881356\n",
            "epoch 789, loss is 0.5803772211074829\n",
            "epoch 789, acc is 0.652542372881356\n",
            "epoch 790, loss is 0.5803675055503845\n",
            "epoch 790, acc is 0.652542372881356\n",
            "epoch 791, loss is 0.5803577899932861\n",
            "epoch 791, acc is 0.652542372881356\n",
            "epoch 792, loss is 0.580348014831543\n",
            "epoch 792, acc is 0.652542372881356\n",
            "epoch 793, loss is 0.5803382992744446\n",
            "epoch 793, acc is 0.652542372881356\n",
            "epoch 794, loss is 0.5803285837173462\n",
            "epoch 794, acc is 0.652542372881356\n",
            "epoch 795, loss is 0.5803188681602478\n",
            "epoch 795, acc is 0.652542372881356\n",
            "epoch 796, loss is 0.5803092122077942\n",
            "epoch 796, acc is 0.652542372881356\n",
            "epoch 797, loss is 0.5802993774414062\n",
            "epoch 797, acc is 0.652542372881356\n",
            "epoch 798, loss is 0.5802897214889526\n",
            "epoch 798, acc is 0.652542372881356\n",
            "epoch 799, loss is 0.5802801251411438\n",
            "epoch 799, acc is 0.652542372881356\n",
            "epoch 800, loss is 0.5802704095840454\n",
            "epoch 800, acc is 0.652542372881356\n",
            "epoch 801, loss is 0.580260694026947\n",
            "epoch 801, acc is 0.652542372881356\n",
            "epoch 802, loss is 0.5802510380744934\n",
            "epoch 802, acc is 0.652542372881356\n",
            "epoch 803, loss is 0.5802413821220398\n",
            "epoch 803, acc is 0.652542372881356\n",
            "epoch 804, loss is 0.5802316665649414\n",
            "epoch 804, acc is 0.652542372881356\n",
            "epoch 805, loss is 0.5802220702171326\n",
            "epoch 805, acc is 0.652542372881356\n",
            "epoch 806, loss is 0.580212414264679\n",
            "epoch 806, acc is 0.652542372881356\n",
            "epoch 807, loss is 0.5802028179168701\n",
            "epoch 807, acc is 0.652542372881356\n",
            "epoch 808, loss is 0.5801931619644165\n",
            "epoch 808, acc is 0.652542372881356\n",
            "epoch 809, loss is 0.5801835656166077\n",
            "epoch 809, acc is 0.652542372881356\n",
            "epoch 810, loss is 0.5801739692687988\n",
            "epoch 810, acc is 0.652542372881356\n",
            "epoch 811, loss is 0.58016437292099\n",
            "epoch 811, acc is 0.652542372881356\n",
            "epoch 812, loss is 0.5801547169685364\n",
            "epoch 812, acc is 0.652542372881356\n",
            "epoch 813, loss is 0.5801451802253723\n",
            "epoch 813, acc is 0.652542372881356\n",
            "epoch 814, loss is 0.5801355838775635\n",
            "epoch 814, acc is 0.652542372881356\n",
            "epoch 815, loss is 0.5801260471343994\n",
            "epoch 815, acc is 0.652542372881356\n",
            "epoch 816, loss is 0.5801163911819458\n",
            "epoch 816, acc is 0.652542372881356\n",
            "epoch 817, loss is 0.5801069140434265\n",
            "epoch 817, acc is 0.652542372881356\n",
            "epoch 818, loss is 0.5800973773002625\n",
            "epoch 818, acc is 0.652542372881356\n",
            "epoch 819, loss is 0.5800878405570984\n",
            "epoch 819, acc is 0.652542372881356\n",
            "epoch 820, loss is 0.5800782442092896\n",
            "epoch 820, acc is 0.652542372881356\n",
            "epoch 821, loss is 0.5800687670707703\n",
            "epoch 821, acc is 0.652542372881356\n",
            "epoch 822, loss is 0.5800592303276062\n",
            "epoch 822, acc is 0.652542372881356\n",
            "epoch 823, loss is 0.5800497531890869\n",
            "epoch 823, acc is 0.652542372881356\n",
            "epoch 824, loss is 0.5800401568412781\n",
            "epoch 824, acc is 0.652542372881356\n",
            "epoch 825, loss is 0.5800306797027588\n",
            "epoch 825, acc is 0.652542372881356\n",
            "epoch 826, loss is 0.5800212025642395\n",
            "epoch 826, acc is 0.652542372881356\n",
            "epoch 827, loss is 0.580011785030365\n",
            "epoch 827, acc is 0.652542372881356\n",
            "epoch 828, loss is 0.5800022482872009\n",
            "epoch 828, acc is 0.652542372881356\n",
            "epoch 829, loss is 0.5799927711486816\n",
            "epoch 829, acc is 0.652542372881356\n",
            "epoch 830, loss is 0.5799833536148071\n",
            "epoch 830, acc is 0.652542372881356\n",
            "epoch 831, loss is 0.5799738764762878\n",
            "epoch 831, acc is 0.652542372881356\n",
            "epoch 832, loss is 0.5799644589424133\n",
            "epoch 832, acc is 0.652542372881356\n",
            "epoch 833, loss is 0.5799550414085388\n",
            "epoch 833, acc is 0.652542372881356\n",
            "epoch 834, loss is 0.5799455642700195\n",
            "epoch 834, acc is 0.652542372881356\n",
            "epoch 835, loss is 0.579936146736145\n",
            "epoch 835, acc is 0.652542372881356\n",
            "epoch 836, loss is 0.5799267292022705\n",
            "epoch 836, acc is 0.652542372881356\n",
            "epoch 837, loss is 0.5799172520637512\n",
            "epoch 837, acc is 0.652542372881356\n",
            "epoch 838, loss is 0.5799078941345215\n",
            "epoch 838, acc is 0.652542372881356\n",
            "epoch 839, loss is 0.5798985362052917\n",
            "epoch 839, acc is 0.652542372881356\n",
            "epoch 840, loss is 0.5798890590667725\n",
            "epoch 840, acc is 0.652542372881356\n",
            "epoch 841, loss is 0.5798797607421875\n",
            "epoch 841, acc is 0.652542372881356\n",
            "epoch 842, loss is 0.579870343208313\n",
            "epoch 842, acc is 0.652542372881356\n",
            "epoch 843, loss is 0.5798609256744385\n",
            "epoch 843, acc is 0.652542372881356\n",
            "epoch 844, loss is 0.5798515677452087\n",
            "epoch 844, acc is 0.652542372881356\n",
            "epoch 845, loss is 0.5798422694206238\n",
            "epoch 845, acc is 0.652542372881356\n",
            "epoch 846, loss is 0.579832911491394\n",
            "epoch 846, acc is 0.652542372881356\n",
            "epoch 847, loss is 0.5798234939575195\n",
            "epoch 847, acc is 0.652542372881356\n",
            "epoch 848, loss is 0.5798141956329346\n",
            "epoch 848, acc is 0.652542372881356\n",
            "epoch 849, loss is 0.5798048377037048\n",
            "epoch 849, acc is 0.652542372881356\n",
            "epoch 850, loss is 0.5797955393791199\n",
            "epoch 850, acc is 0.652542372881356\n",
            "epoch 851, loss is 0.5797862410545349\n",
            "epoch 851, acc is 0.652542372881356\n",
            "epoch 852, loss is 0.57977694272995\n",
            "epoch 852, acc is 0.652542372881356\n",
            "epoch 853, loss is 0.5797675848007202\n",
            "epoch 853, acc is 0.652542372881356\n",
            "epoch 854, loss is 0.5797582864761353\n",
            "epoch 854, acc is 0.652542372881356\n",
            "epoch 855, loss is 0.5797490477561951\n",
            "epoch 855, acc is 0.652542372881356\n",
            "epoch 856, loss is 0.5797396898269653\n",
            "epoch 856, acc is 0.652542372881356\n",
            "epoch 857, loss is 0.5797305107116699\n",
            "epoch 857, acc is 0.652542372881356\n",
            "epoch 858, loss is 0.579721212387085\n",
            "epoch 858, acc is 0.652542372881356\n",
            "epoch 859, loss is 0.5797119140625\n",
            "epoch 859, acc is 0.652542372881356\n",
            "epoch 860, loss is 0.5797027349472046\n",
            "epoch 860, acc is 0.652542372881356\n",
            "epoch 861, loss is 0.5796934366226196\n",
            "epoch 861, acc is 0.652542372881356\n",
            "epoch 862, loss is 0.5796841979026794\n",
            "epoch 862, acc is 0.652542372881356\n",
            "epoch 863, loss is 0.5796749591827393\n",
            "epoch 863, acc is 0.652542372881356\n",
            "epoch 864, loss is 0.5796657800674438\n",
            "epoch 864, acc is 0.652542372881356\n",
            "epoch 865, loss is 0.5796564817428589\n",
            "epoch 865, acc is 0.652542372881356\n",
            "epoch 866, loss is 0.5796472430229187\n",
            "epoch 866, acc is 0.652542372881356\n",
            "epoch 867, loss is 0.5796380639076233\n",
            "epoch 867, acc is 0.652542372881356\n",
            "epoch 868, loss is 0.5796288847923279\n",
            "epoch 868, acc is 0.652542372881356\n",
            "epoch 869, loss is 0.5796197056770325\n",
            "epoch 869, acc is 0.652542372881356\n",
            "epoch 870, loss is 0.5796104669570923\n",
            "epoch 870, acc is 0.652542372881356\n",
            "epoch 871, loss is 0.5796012878417969\n",
            "epoch 871, acc is 0.652542372881356\n",
            "epoch 872, loss is 0.5795921683311462\n",
            "epoch 872, acc is 0.652542372881356\n",
            "epoch 873, loss is 0.5795829892158508\n",
            "epoch 873, acc is 0.652542372881356\n",
            "epoch 874, loss is 0.5795738101005554\n",
            "epoch 874, acc is 0.652542372881356\n",
            "epoch 875, loss is 0.57956463098526\n",
            "epoch 875, acc is 0.652542372881356\n",
            "epoch 876, loss is 0.5795554518699646\n",
            "epoch 876, acc is 0.652542372881356\n",
            "epoch 877, loss is 0.579546332359314\n",
            "epoch 877, acc is 0.652542372881356\n",
            "epoch 878, loss is 0.5795372128486633\n",
            "epoch 878, acc is 0.652542372881356\n",
            "epoch 879, loss is 0.5795281529426575\n",
            "epoch 879, acc is 0.652542372881356\n",
            "epoch 880, loss is 0.5795189738273621\n",
            "epoch 880, acc is 0.652542372881356\n",
            "epoch 881, loss is 0.5795098543167114\n",
            "epoch 881, acc is 0.652542372881356\n",
            "epoch 882, loss is 0.5795007348060608\n",
            "epoch 882, acc is 0.652542372881356\n",
            "epoch 883, loss is 0.5794915556907654\n",
            "epoch 883, acc is 0.652542372881356\n",
            "epoch 884, loss is 0.5794824957847595\n",
            "epoch 884, acc is 0.652542372881356\n",
            "epoch 885, loss is 0.5794734358787537\n",
            "epoch 885, acc is 0.652542372881356\n",
            "epoch 886, loss is 0.5794643759727478\n",
            "epoch 886, acc is 0.652542372881356\n",
            "epoch 887, loss is 0.5794553160667419\n",
            "epoch 887, acc is 0.652542372881356\n",
            "epoch 888, loss is 0.5794462561607361\n",
            "epoch 888, acc is 0.652542372881356\n",
            "epoch 889, loss is 0.5794371366500854\n",
            "epoch 889, acc is 0.652542372881356\n",
            "epoch 890, loss is 0.5794280171394348\n",
            "epoch 890, acc is 0.652542372881356\n",
            "epoch 891, loss is 0.5794190168380737\n",
            "epoch 891, acc is 0.652542372881356\n",
            "epoch 892, loss is 0.5794099569320679\n",
            "epoch 892, acc is 0.652542372881356\n",
            "epoch 893, loss is 0.579400897026062\n",
            "epoch 893, acc is 0.652542372881356\n",
            "epoch 894, loss is 0.5793918967247009\n",
            "epoch 894, acc is 0.652542372881356\n",
            "epoch 895, loss is 0.5793828368186951\n",
            "epoch 895, acc is 0.652542372881356\n",
            "epoch 896, loss is 0.5793738961219788\n",
            "epoch 896, acc is 0.652542372881356\n",
            "epoch 897, loss is 0.5793648362159729\n",
            "epoch 897, acc is 0.652542372881356\n",
            "epoch 898, loss is 0.5793558359146118\n",
            "epoch 898, acc is 0.652542372881356\n",
            "epoch 899, loss is 0.579346776008606\n",
            "epoch 899, acc is 0.652542372881356\n",
            "epoch 900, loss is 0.5793378353118896\n",
            "epoch 900, acc is 0.652542372881356\n",
            "epoch 901, loss is 0.5793288350105286\n",
            "epoch 901, acc is 0.652542372881356\n",
            "epoch 902, loss is 0.5793198347091675\n",
            "epoch 902, acc is 0.652542372881356\n",
            "epoch 903, loss is 0.5793108940124512\n",
            "epoch 903, acc is 0.652542372881356\n",
            "epoch 904, loss is 0.5793018937110901\n",
            "epoch 904, acc is 0.652542372881356\n",
            "epoch 905, loss is 0.5792929530143738\n",
            "epoch 905, acc is 0.652542372881356\n",
            "epoch 906, loss is 0.5792840123176575\n",
            "epoch 906, acc is 0.652542372881356\n",
            "epoch 907, loss is 0.5792750120162964\n",
            "epoch 907, acc is 0.652542372881356\n",
            "epoch 908, loss is 0.5792660713195801\n",
            "epoch 908, acc is 0.652542372881356\n",
            "epoch 909, loss is 0.5792571306228638\n",
            "epoch 909, acc is 0.652542372881356\n",
            "epoch 910, loss is 0.5792481899261475\n",
            "epoch 910, acc is 0.652542372881356\n",
            "epoch 911, loss is 0.5792393088340759\n",
            "epoch 911, acc is 0.652542372881356\n",
            "epoch 912, loss is 0.5792303085327148\n",
            "epoch 912, acc is 0.652542372881356\n",
            "epoch 913, loss is 0.5792214274406433\n",
            "epoch 913, acc is 0.652542372881356\n",
            "epoch 914, loss is 0.579212486743927\n",
            "epoch 914, acc is 0.652542372881356\n",
            "epoch 915, loss is 0.5792036056518555\n",
            "epoch 915, acc is 0.652542372881356\n",
            "epoch 916, loss is 0.5791947245597839\n",
            "epoch 916, acc is 0.652542372881356\n",
            "epoch 917, loss is 0.5791857838630676\n",
            "epoch 917, acc is 0.652542372881356\n",
            "epoch 918, loss is 0.5791769623756409\n",
            "epoch 918, acc is 0.652542372881356\n",
            "epoch 919, loss is 0.5791680216789246\n",
            "epoch 919, acc is 0.652542372881356\n",
            "epoch 920, loss is 0.5791590809822083\n",
            "epoch 920, acc is 0.652542372881356\n",
            "epoch 921, loss is 0.5791503190994263\n",
            "epoch 921, acc is 0.652542372881356\n",
            "epoch 922, loss is 0.5791414380073547\n",
            "epoch 922, acc is 0.652542372881356\n",
            "epoch 923, loss is 0.579132616519928\n",
            "epoch 923, acc is 0.652542372881356\n",
            "epoch 924, loss is 0.5791237950325012\n",
            "epoch 924, acc is 0.652542372881356\n",
            "epoch 925, loss is 0.5791148543357849\n",
            "epoch 925, acc is 0.652542372881356\n",
            "epoch 926, loss is 0.5791060924530029\n",
            "epoch 926, acc is 0.652542372881356\n",
            "epoch 927, loss is 0.5790972113609314\n",
            "epoch 927, acc is 0.652542372881356\n",
            "epoch 928, loss is 0.5790883898735046\n",
            "epoch 928, acc is 0.652542372881356\n",
            "epoch 929, loss is 0.5790795683860779\n",
            "epoch 929, acc is 0.652542372881356\n",
            "epoch 930, loss is 0.5790708065032959\n",
            "epoch 930, acc is 0.652542372881356\n",
            "epoch 931, loss is 0.5790619850158691\n",
            "epoch 931, acc is 0.652542372881356\n",
            "epoch 932, loss is 0.5790531635284424\n",
            "epoch 932, acc is 0.652542372881356\n",
            "epoch 933, loss is 0.5790443420410156\n",
            "epoch 933, acc is 0.652542372881356\n",
            "epoch 934, loss is 0.5790356397628784\n",
            "epoch 934, acc is 0.652542372881356\n",
            "epoch 935, loss is 0.5790268182754517\n",
            "epoch 935, acc is 0.652542372881356\n",
            "epoch 936, loss is 0.5790180563926697\n",
            "epoch 936, acc is 0.652542372881356\n",
            "epoch 937, loss is 0.5790092349052429\n",
            "epoch 937, acc is 0.652542372881356\n",
            "epoch 938, loss is 0.5790004134178162\n",
            "epoch 938, acc is 0.652542372881356\n",
            "epoch 939, loss is 0.5789916515350342\n",
            "epoch 939, acc is 0.652542372881356\n",
            "epoch 940, loss is 0.5789830088615417\n",
            "epoch 940, acc is 0.652542372881356\n",
            "epoch 941, loss is 0.578974187374115\n",
            "epoch 941, acc is 0.652542372881356\n",
            "epoch 942, loss is 0.5789654850959778\n",
            "epoch 942, acc is 0.652542372881356\n",
            "epoch 943, loss is 0.5789567232131958\n",
            "epoch 943, acc is 0.652542372881356\n",
            "epoch 944, loss is 0.5789480209350586\n",
            "epoch 944, acc is 0.652542372881356\n",
            "epoch 945, loss is 0.5789392590522766\n",
            "epoch 945, acc is 0.652542372881356\n",
            "epoch 946, loss is 0.5789305567741394\n",
            "epoch 946, acc is 0.652542372881356\n",
            "epoch 947, loss is 0.5789217948913574\n",
            "epoch 947, acc is 0.652542372881356\n",
            "epoch 948, loss is 0.578913152217865\n",
            "epoch 948, acc is 0.652542372881356\n",
            "epoch 949, loss is 0.5789044499397278\n",
            "epoch 949, acc is 0.652542372881356\n",
            "epoch 950, loss is 0.5788956880569458\n",
            "epoch 950, acc is 0.652542372881356\n",
            "epoch 951, loss is 0.5788870453834534\n",
            "epoch 951, acc is 0.652542372881356\n",
            "epoch 952, loss is 0.5788783431053162\n",
            "epoch 952, acc is 0.652542372881356\n",
            "epoch 953, loss is 0.5788697004318237\n",
            "epoch 953, acc is 0.652542372881356\n",
            "epoch 954, loss is 0.5788610577583313\n",
            "epoch 954, acc is 0.652542372881356\n",
            "epoch 955, loss is 0.5788523554801941\n",
            "epoch 955, acc is 0.652542372881356\n",
            "epoch 956, loss is 0.5788436532020569\n",
            "epoch 956, acc is 0.652542372881356\n",
            "epoch 957, loss is 0.5788350701332092\n",
            "epoch 957, acc is 0.652542372881356\n",
            "epoch 958, loss is 0.5788263082504272\n",
            "epoch 958, acc is 0.652542372881356\n",
            "epoch 959, loss is 0.5788177251815796\n",
            "epoch 959, acc is 0.652542372881356\n",
            "epoch 960, loss is 0.5788090825080872\n",
            "epoch 960, acc is 0.652542372881356\n",
            "epoch 961, loss is 0.57880038022995\n",
            "epoch 961, acc is 0.652542372881356\n",
            "epoch 962, loss is 0.5787917971611023\n",
            "epoch 962, acc is 0.652542372881356\n",
            "epoch 963, loss is 0.5787832140922546\n",
            "epoch 963, acc is 0.652542372881356\n",
            "epoch 964, loss is 0.5787745118141174\n",
            "epoch 964, acc is 0.652542372881356\n",
            "epoch 965, loss is 0.5787659287452698\n",
            "epoch 965, acc is 0.652542372881356\n",
            "epoch 966, loss is 0.5787573456764221\n",
            "epoch 966, acc is 0.652542372881356\n",
            "epoch 967, loss is 0.5787487626075745\n",
            "epoch 967, acc is 0.652542372881356\n",
            "epoch 968, loss is 0.578740119934082\n",
            "epoch 968, acc is 0.652542372881356\n",
            "epoch 969, loss is 0.5787315368652344\n",
            "epoch 969, acc is 0.652542372881356\n",
            "epoch 970, loss is 0.5787229537963867\n",
            "epoch 970, acc is 0.652542372881356\n",
            "epoch 971, loss is 0.5787144303321838\n",
            "epoch 971, acc is 0.652542372881356\n",
            "epoch 972, loss is 0.5787057876586914\n",
            "epoch 972, acc is 0.652542372881356\n",
            "epoch 973, loss is 0.5786972045898438\n",
            "epoch 973, acc is 0.652542372881356\n",
            "epoch 974, loss is 0.5786886811256409\n",
            "epoch 974, acc is 0.652542372881356\n",
            "epoch 975, loss is 0.578680157661438\n",
            "epoch 975, acc is 0.652542372881356\n",
            "epoch 976, loss is 0.5786715149879456\n",
            "epoch 976, acc is 0.652542372881356\n",
            "epoch 977, loss is 0.5786629319190979\n",
            "epoch 977, acc is 0.652542372881356\n",
            "epoch 978, loss is 0.5786544680595398\n",
            "epoch 978, acc is 0.652542372881356\n",
            "epoch 979, loss is 0.5786459445953369\n",
            "epoch 979, acc is 0.652542372881356\n",
            "epoch 980, loss is 0.578637421131134\n",
            "epoch 980, acc is 0.652542372881356\n",
            "epoch 981, loss is 0.5786288380622864\n",
            "epoch 981, acc is 0.652542372881356\n",
            "epoch 982, loss is 0.5786203742027283\n",
            "epoch 982, acc is 0.652542372881356\n",
            "epoch 983, loss is 0.5786118507385254\n",
            "epoch 983, acc is 0.652542372881356\n",
            "epoch 984, loss is 0.5786033272743225\n",
            "epoch 984, acc is 0.652542372881356\n",
            "epoch 985, loss is 0.5785948038101196\n",
            "epoch 985, acc is 0.652542372881356\n",
            "epoch 986, loss is 0.5785863995552063\n",
            "epoch 986, acc is 0.652542372881356\n",
            "epoch 987, loss is 0.5785778760910034\n",
            "epoch 987, acc is 0.652542372881356\n",
            "epoch 988, loss is 0.5785692930221558\n",
            "epoch 988, acc is 0.652542372881356\n",
            "epoch 989, loss is 0.5785608291625977\n",
            "epoch 989, acc is 0.652542372881356\n",
            "epoch 990, loss is 0.5785523653030396\n",
            "epoch 990, acc is 0.652542372881356\n",
            "epoch 991, loss is 0.5785439014434814\n",
            "epoch 991, acc is 0.652542372881356\n",
            "epoch 992, loss is 0.5785354375839233\n",
            "epoch 992, acc is 0.652542372881356\n",
            "epoch 993, loss is 0.5785269737243652\n",
            "epoch 993, acc is 0.652542372881356\n",
            "epoch 994, loss is 0.5785185098648071\n",
            "epoch 994, acc is 0.652542372881356\n",
            "epoch 995, loss is 0.5785101056098938\n",
            "epoch 995, acc is 0.652542372881356\n",
            "epoch 996, loss is 0.5785017013549805\n",
            "epoch 996, acc is 0.652542372881356\n",
            "epoch 997, loss is 0.5784932374954224\n",
            "epoch 997, acc is 0.652542372881356\n",
            "epoch 998, loss is 0.5784847736358643\n",
            "epoch 998, acc is 0.652542372881356\n",
            "epoch 999, loss is 0.5784763693809509\n",
            "epoch 999, acc is 0.652542372881356\n"
          ]
        }
      ]
    }
  ]
}